\documentclass{report}
\usepackage{framed}
\usepackage[outerbars]{changebar}

% Dimensions de la page
\usepackage{geometry}
%\geometry{scale=0.8}
\geometry{a4paper}

% Acronyms
\usepackage[nonumberlist]{glossaries}
\setacronymstyle{long-short}
\makenoidxglossaries
% Load acronyms list
\loadglsentries{acronyms}

% Graphics
\usepackage{graphicx}
\graphicspath{{Figures/}}

% Dimensions de la page
\usepackage{geometry}
%\geometry{scale=0.8}
\geometry{a4paper}

% Acronyms
\usepackage[nonumberlist]{glossaries}
\setacronymstyle{long-short}
\makenoidxglossaries
% Load acronyms list
\loadglsentries{acronyms}

% Graphics
\usepackage{graphicx}
\graphicspath{{Figures/}}

% Prettyref
\usepackage{prettyref}
\usepackage{hyperref}

\title{Data representation}
\author{Aciditeam}
\begin{document}
\maketitle

\section{Pianoroll representation}
\subsection{Binary pianoroll}
The first and most common data representation we can think about is a matrix of binary units indicating whether a note is on or off. In this case, the network would use binary units.

\subsection{Dynamic pianoroll}
The first improvement we can make is to take dynamics into consideration. I think that dynamics can provide a useful information about the position of beat/down-beats and be used to infer a rhythmic structure.

This imply using a network with real valued input units, either by considering probabilities as the final value, or even using Gaussian or preferably ReLu units.

\section{Braids}
Using the braids representation (cf Mattia).
Still to be read, be very promising in order to obtain a compact meaningful representation. A priori rely on real units.

\section{From sparse binary encoding to compact distributional representations}
Encoding frames of symbolic music with a binary vector lead to sparse representations that I think is not the most suited in order to discover an underlying structure. In \gls*{NLP}, the same problem arises when dictionary are encoded in a 1-to-D way, which is associating one unit to a word. It leads to highly sparse representation (one unit is active on a vector of the size of the dictionary !). A simple yet efficient solution has been proposed by Mikolov, \cite{mikolov2013efficient,mikolov2013distributed}. It consists in making a network discover an adapted representation through a predictive task. The Skip-gram model, most efficient up to now, consists more specifically to \textit{find representations that are useful for predicting the surrounding words in a sentence or a document}.
Those predicted words are determined through a projection (representation) layer and a softmax layer to choose the next word. Since the number of words is huge, a simple softmax structure is computationally exhausting. Hence a nice solution is to use a hierarchical softmax layer which is a classifier adapted to Huffman coding of word and require only $\log_{2}(D)$ operations instead of $D$ to find the occurrence probability of a word given the input word.

I eventually don't think that those kind of representation in the most adapted for symoblic music since the vector size is not that huge neither sparse in our case, and that intensity information should be kept.

\end{document}
