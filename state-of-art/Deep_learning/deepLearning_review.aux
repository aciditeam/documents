\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{humphrey2013feature}
\citation{casey2008content}
\citation{ellis2012large}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{hupe1998cortical}
\citation{utgoff2002many}
\citation{hinton2006fast}
\citation{le2013building}
\citation{bengio2009learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Advantages of deep architectures}{3}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Feature learning}{3}{subsection.1.2}}
\citation{bengio2013representation}
\citation{bengio2013representation}
\citation{weston2012deep}
\citation{rifai2011contractive}
\citation{yu2011deep}
\citation{humphrey2013feature}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Manifold and complexity}{4}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Application to audio}{4}{subsection.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Time scales problem}{5}{subsubsection.1.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Architectures}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Logistic regression}{5}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An artificial neuron basically output the mapping of its input $z$ by a function $f$ }}{6}{figure.1}}
\newlabel{fig:neuron-unit}{{1}{6}{An artificial neuron basically output the mapping of its input $z$ by a function $f$}{figure.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Properties}{6}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural networks}{6}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Artificial neuron}{6}{subsubsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Various computation units}{6}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Sigmoid}{6}{section*.3}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Hyperbolic tangent}{6}{section*.4}}
\newlabel{fig:Different_type_of_unit}{{2.2.2}{7}{Various computation units}{subsubsection.2.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gaussian}{7}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Rectified linear}{7}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Others}{7}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Multi-layer neural networks}{7}{subsubsection.2.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Multi-layer neural network with directed bottom-up connections}}{8}{figure.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Learning algorithm}{8}{subsubsection.2.2.4}}
\citation{duchi2011adaptive}
\citation{liu1989limited}
\citation{yu2011deep}
\citation{bengio2007greedy}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Gradient descent}{9}{subsubsection.2.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Difficulty of training deep architectures}{9}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Unsupervised (self-taught) learning}{9}{subsection.2.4}}
\citation{hinton2006fast}
\citation{erhan2009difficulty}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Greedy layer-wise training}}{10}{figure.3}}
\newlabel{fig:layer-wise_training}{{3}{10}{Greedy layer-wise training}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Greedy layer-wise training}{10}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Reconstruction goal}{10}{subsection.2.6}}
\citation{vincent2011connection}
\citation{rumelhart1988learning}
\citation{baldi1989neural}
\@writefile{toc}{\contentsline {section}{\numberline {3}Single-layer modules}{11}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Auto-encoders}{11}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Basic auto-encoder}{11}{subsubsection.3.1.1}}
\citation{bengio2007greedy}
\newlabel{fig:auto-encoders}{{3.1.1}{12}{Basic auto-encoder}{subsubsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An auto-encoder can be seen as an encoding-decoding function}}{12}{figure.4}}
\citation{ranzato2007sparse}
\citation{lee2008sparse}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sparse auto-encoder. Sparsity constraints over the hidden layer : only a small number of unit are activated for a given observation}}{13}{figure.5}}
\newlabel{fig:sparse-auto-encoder}{{5}{13}{Sparse auto-encoder. Sparsity constraints over the hidden layer : only a small number of unit are activated for a given observation}{figure.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Regularized auto-encoders}{13}{subsubsection.3.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Sparse auto-encoders}{13}{subsubsection.3.1.3}}
\citation{vincent2010stacked}
\citation{rifai2011contractive}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The denoising auto-encoder try to output the best reconstruction of the input from a corrupted version of this input.}}{14}{figure.6}}
\newlabel{fig:denoising-auto-encoder}{{6}{14}{The denoising auto-encoder try to output the best reconstruction of the input from a corrupted version of this input}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Denoising auto-encoders}{14}{subsubsection.3.1.4}}
\citation{rifai2011higher}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Contractive auto-encoders}{15}{subsubsection.3.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Linear decoders}{15}{subsubsection.3.1.6}}
\citation{yu2011deep}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A \textit  {Restricted Boltzmann Machine}. One can notice there is no connection between units of the same layer (meaning there are independent).}}{16}{figure.7}}
\newlabel{fig:RBM}{{7}{16}{A \textit {Restricted Boltzmann Machine}. One can notice there is no connection between units of the same layer (meaning there are independent)}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Restricted Boltzmann Machine}{16}{subsection.3.2}}
\citation{hinton2006fast}
\citation{hinton2010practical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Different types of unit}{17}{subsubsection.3.2.1}}
\citation{teh2001rate}
\citation{taylor2006modeling}
\citation{taylor2009factored}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Sigmo\IeC {\"\i }d and softmax units}{18}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gaussian units}{18}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Binomial units}{18}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Rectified linear units}{18}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Conditional RBM}{18}{subsubsection.3.2.2}}
\citation{sutskever2007learning}
\citation{memisevic2007unsupervised}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Conditional RBM}}{19}{figure.8}}
\newlabel{fig:Conditional_RBM}{{8}{19}{Conditional RBM}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Temporal RBM}{19}{subsubsection.3.2.3}}
\citation{mnih2007three}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Gated RBM}{20}{subsubsection.3.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Factored RBM}{20}{subsubsection.3.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Deep architectures}{20}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Stacked auto-encoders}{20}{subsection.4.1}}
\citation{hinton2006fast}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Pre-training stacked autoencoders}{21}{subsubsection.4.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Fine-tuning stacked auto-encoders}{21}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Deep Belief Networks}{21}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Greedy layer-wise training of a stacked auto-encoder}}{22}{figure.9}}
\newlabel{fig:Greedy_layer-wise_training}{{9}{22}{Greedy layer-wise training of a stacked auto-encoder}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textit  {Deep Belief Network} architecture. Except in the top layer which is an RBM, the connections are top-down directed connections.}}{23}{figure.10}}
\newlabel{fig:DBN_archi}{{10}{23}{\textit {Deep Belief Network} architecture. Except in the top layer which is an RBM, the connections are top-down directed connections}{figure.10}{}}
\citation{hinton2006fast}
\citation{yu2011deep}
\citation{tang2010deep}
\citation{salakhutdinov2009deep}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces DBN greedy layer-wise training}}{24}{figure.11}}
\newlabel{fig:DBN_training}{{11}{24}{DBN greedy layer-wise training}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Deep Boltzmann Machine}{24}{subsection.4.3}}
\citation{salakhutdinov2009deep}
\citation{langkvist2014review}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textit  {Deep Boltzmann Machine.} Connections are undirected. Inference of the probability activation of a unit given its top and bottom layers is illustrated by the red and blue arrows}}{25}{figure.12}}
\newlabel{fig:DBM_sample}{{12}{25}{\textit {Deep Boltzmann Machine.} Connections are undirected. Inference of the probability activation of a unit given its top and bottom layers is illustrated by the red and blue arrows}{figure.12}{}}
\citation{pineda1987generalization}
\citation{werbos1990backpropagation}
\citation{sutskever2007learning}
\citation{hochreiter1997long}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces General architecture for a \textit  {Recurrent neural network}}}{26}{figure.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Temporal models}{26}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Recurrent Neural Network}{26}{subsubsection.4.4.1}}
\citation{masci2011stacked}
\citation{lee2009convolutional}
\citation{waibel1989modular}
\citation{lee2009convolutional}
\citation{chen2010deep}
\citation{kavukcuoglu2009learning}
\citation{gers2000learning}
\citation{martens2012training}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Convolution and pooling}{27}{subsubsection.4.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Temporal coherence}{27}{subsubsection.4.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.4}Comparison}{27}{subsubsection.4.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.5}Summary}{27}{subsubsection.4.4.5}}
\citation{bottou2010large}
\citation{humphrey2013feature}
\citation{hubel1968receptive}
\citation{lecun1998gradient}
\@writefile{toc}{\contentsline {section}{\numberline {5}Other models}{28}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Convolutional Neural Networks}{28}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Sparse connectivity}{28}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Convolutions}{28}{subsubsection.5.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \textit  {Sparse connectivity.} The unit in the layer $n$ is connected to a small number of unit from the layer $n-1$}}{29}{figure.14}}
\newlabel{fig:Sparse_connectivity}{{14}{29}{\textit {Sparse connectivity.} The unit in the layer $n$ is connected to a small number of unit from the layer $n-1$}{figure.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Shared weights}{29}{subsubsection.5.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Feature maps}{29}{subsubsection.5.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \textit  {Shared weights.} The same weights are replicated over the inputs of the hidden units}}{30}{figure.15}}
\newlabel{fig:Shared_weights}{{15}{30}{\textit {Shared weights.} The same weights are replicated over the inputs of the hidden units}{figure.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Max pooling}{30}{subsubsection.5.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.6}Tying the full model together}{30}{subsubsection.5.1.6}}
\citation{olshausen1997sparse}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.7}Choosing hyperparameters}{31}{subsubsection.5.1.7}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Filters}{31}{section*.12}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Pooling}{31}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Sparse coding}{31}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Probabilistic interpretation}{31}{subsubsection.5.2.1}}
\citation{goodfellow2012large}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Autoencoder interpretation}{32}{subsubsection.5.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Topographic sparse coding}{33}{subsubsection.5.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Deconvolutional networks}{33}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Single layer}{33}{subsubsection.5.3.1}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gaussian unpooling}{34}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Cost function}{34}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Single layer inference}{34}{section*.16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Multi-layer stacking}{34}{subsubsection.5.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Applying deep learning}{34}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Preprocessing}{34}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}PCA}{35}{subsubsection.6.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Stationarity}{35}{subsubsection.6.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Whitening}{35}{subsubsection.6.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.4}ZCA Whitening}{35}{subsubsection.6.1.4}}
\citation{lee2008sparse}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.5}Pre-processing parameters}{36}{subsubsection.6.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Hyper-parameters analysis}{36}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}The learning rate}{36}{subsubsection.6.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Initial weights values}{36}{subsubsection.6.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Momentum}{36}{subsubsection.6.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Weight decay}{36}{subsubsection.6.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.5}Held-out validation data}{36}{subsubsection.6.2.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.6}Encouraging sparse hidden activities}{36}{subsubsection.6.2.6}}
\citation{saxe2011random}
\citation{hinton2006fast}
\citation{carreira2005contrastive}
\citation{salakhutdinov2007restricted}
\citation{tieleman2008training}
\citation{tieleman2009using}
\citation{hinton2010practical}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.7}Number of hidden units}{37}{subsubsection.6.2.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.8}Varieties of contrastive divergence}{37}{subsubsection.6.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.9}The size of a mini-batch}{37}{subsubsection.6.2.9}}
\citation{yu2011deep}
\citation{mnih2015human}
\citation{deng2010binary}
\citation{nair20093d}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Parameter tuning search}{38}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Monitoring and displaying}{38}{subsection.6.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Monitoring the learning}{38}{subsubsection.6.4.1}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Histograms}{38}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Specificity of the features}{38}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Weights display}{38}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}t-distributed Stochastic Neighbor Embedding (t-SNE)}{38}{subsubsection.6.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Applications}{38}{section.7}}
\citation{yu2011deep}
\citation{humphrey2013feature}
\citation{humphrey2012rethinking}
\@writefile{toc}{\contentsline {section}{\numberline {8}Future directions}{39}{section.8}}
\citation{rifai2011contractive}
\citation{hinton2006fast}
\citation{bengio2007greedy}
\citation{olshausen1997sparse}
\citation{kavukcuoglu2009learning}
\bibstyle{plain}
\bibdata{deeplearning}
\@writefile{toc}{\contentsline {section}{\numberline {9}General infos}{40}{section.9}}
