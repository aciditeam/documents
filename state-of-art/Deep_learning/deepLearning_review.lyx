#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass scrartcl
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1cm
\topmargin 1cm
\rightmargin 1cm
\bottommargin 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Deep learning review
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The challenges of Artificial Intelligence (AI) is to seek that a computer
 could exhibit signs of what we dub as intelligence.
 The field of 
\emph on
perceptual
\emph default
 AI is itself based on the hypothesis that it could be possible to teach
 a machine to experience and respond similarly to perceptual events (such
 as musical events) as a human expert would 
\begin_inset CommandInset citation
LatexCommand cite
key "humphrey2013feature"

\end_inset

.
 Even though this stimulating field raised interest for several decades,
 as the notion of 
\emph on
intelligence
\emph default
 seems daunting to define, researchers focused on the notions of 
\emph on
machine learning 
\emph default
where a computer could learn to perform a specific set of tasks.
 Despite ongoing efforts in all information retrieval and data mining fields,
 most of the consistently evaluated tasks seem to be converging towards
 pervasive performance ceilings across many open questions in each respective
 discipline, which stands under expected accuracies 
\begin_inset CommandInset citation
LatexCommand cite
key "casey2008content"

\end_inset

.
 Furthermore, it has been repeatedly shown that the performance of most
 state-of-art algorithms deteriorate significantly when applied to more
 realistic and larger datasets 
\begin_inset CommandInset citation
LatexCommand cite
key "ellis2012large"

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
=> Common techniques build on trying to decompose into sub-problems
\end_layout

\begin_layout Standard

\series bold
=> Principle of increasingly complex abstractions
\end_layout

\begin_layout Standard
Most current machine learning techniques rely on shallow architectures,
 where (notwithstanding eventual data pre-processing), only a single layer
 of nonlinear transformation is applied to a set of features that are in
 turn fed to a discriminative pattern recognition algorithm.
 However, research in human cognition 
\begin_inset CommandInset citation
LatexCommand cite
key "hupe1998cortical"

\end_inset

 suggest that deep architectures might be implemented by human perception
 mechanisms for extracting complex structure out of raw information.
 Hence this depth of processing would seem like a natural choice for building
 internal representation of increasingly higher-level concepts.
 These types of clearly layered hierarchical structures seems logical by
 simply thinking about our abilities to decipher and transform information
 spanning from the raw input level up to the linguistic and even paradigmatic
 analysis level.
\end_layout

\begin_layout Standard

\series bold
=> Limits of features
\end_layout

\begin_layout Standard
Several authors pointed out the sub-optimality and limitations of hand-designing
 features.
 Furthermore, current learning systems are also bound to shallow architectures
 which are inherently limited.
\end_layout

\begin_layout Standard
Deep learning is based on the hypothesis that each layer of processing tries
 to remove a different kind of variance by exploiting the statistical regulariti
es of the current level of abstraction.
 Deep architectures can be seen as a set of increasingly higher-level abstractio
ns that decompose a complex problems into a hierarchical array of simpler
 ones.
 Many researchers tried to train deep architectures for decades without
 any success 
\begin_inset CommandInset citation
LatexCommand cite
key "utgoff2002many"

\end_inset

.
 However, interpreting each layer as a singular problem of finding optimal
 regularities to move from one level of abstraction to a higher-level one
 allows to decompose the deep learning in itself.
 This leads to one of the core component in the success of current deep
 learning approaches being the use of a 
\emph on
greedy layer-wise unsupervised training
\emph default
 which was introduced in the work of Hinton et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

.
 This allows to train each layer separately, alleviating the problems of
 training deep architecture through a method with a time complexity only
 linear with the depth of the network.
\end_layout

\begin_layout Standard
This hierarchy of non-linear processing layers provides a workflow in which
 the outputs of each layer provide a certain level of abstraction which
 is in turn fed to its higher layer as an input to seek an even higher-level
 abstraction for extracting structures by exploiting statistical regularities
 in this input.
\end_layout

\begin_layout Standard

\series bold
=> Deep learning can be seen as transforming a representation into another
 (+ distributed representation)
\end_layout

\begin_layout Standard
Manually crafted features are inherently limited, if not only by the knowledge
 of their designer, but also in their restricted ability to generalize and
 adapt to different problems and datasets.
 Furthermore, shallow architectures are fundamentally limited by the very
 nature of their structure which cannot leverage different granularity of
 knowledge representation.
\end_layout

\begin_layout Standard
Recent works 
\begin_inset CommandInset citation
LatexCommand cite
key "le2013building"

\end_inset

 even try to adapt these ideas to large-scale datasets and massive amounts
 of data processing.
\end_layout

\begin_layout Subsection
Advantages of deep architectures
\end_layout

\begin_layout Standard
Most machine learning approaches have gradually converged towards a deep
 architecture.
 Indeed, if each transformation of the data is thought of as a layer of
 processing, then we can see that most research is based on the gradual
 expansion of systems with additionnal steps.
 By reducing these operations to the three major types of affine (linear)
 combination, non-linear mapping and grouping (or pooling), we can see that
 all methods can be cast into the framework of deep architectures.
 Deep learning can be seen as a generalization of current research if common
 processing structures are seen as transformative layer (filtering, non-linearit
y and grouping).
\end_layout

\begin_layout Standard
The number of parallel units required in a transformation function defines
 its breadth 
\series bold
[xREFx]
\series default
.
 There is a clear trade-off between the breadth of a structure and its depth.
 
\end_layout

\begin_layout Standard
Deep architectures also exploit the characteristic of 
\emph on
emergence 
\emph default
which appears where the combination of simpler elements creates a higher-level
 composed object that is superior to the sum of its parts.
 This plays a fundamental role in the representational power of deep structures
 as the ability to perform multiple non-linear combination of small components
 lead to more versatile expressivity.
 In this endeavor, the use of non-linearity at each level is the key aspect
 of the expressive power of deep architectures.
 Indeed, a combination of linear operations of any depth can always be reduced
 to a linear transformation, whereas non-linear operation provide increasingly
 complex interaction as a function of depth.
 This property of deep architectures produce an exponential growth of different
 processing 
\emph on
pathways 
\emph default
which provides variety of paths reusing their various computational parts
 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2009learning"

\end_inset

.
\end_layout

\begin_layout Subsection
Feature learning
\end_layout

\begin_layout Standard
The consensus growing through the past decade of machine learning has shown
 the fundamental importance of powerful and expressive features representing
 the data.
 In the common two-stage supervised learning paradigm of first extracting
 features from the raw input and then providing a semantic interpretation,
 it would even seem that a naive classifier is sufficient as long as the
 features are optimal for the task at hand.
 This is driven by the simple observation that irrelevant variations or
 noise embedded in a feature must be alleviated through the pattern recognizer
 instead.
 Hence, the more accurate and robust is a feature and the simpler the pattern
 matching approach needs to be (and conversely).
\end_layout

\begin_layout Standard
The expressivity of a representation is highly tied to its ability to its
 understand and disentangle the underlying factors of variation.
 Hence, a good representation is targeted to simultaneously remove irrelevant
 variance and emphasize the critical explanatory variables hidden in the
 low-level data 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2013representation"

\end_inset

.
 This in turn imply that a good representation will provide a high similarity
 between two elements with similar abstract features, even though their
 raw values are distant.
\end_layout

\begin_layout Standard
Traditionnaly, features are designed by leveraging an intellectual expectation
 on what characteristics should be extracted from an input and trying to
 reify this concept.
 This expectation is usually based on what explicit concept we seek to determine
 and discriminate.
 Therefore, hand-crafted features are bound to the representational expressivity
 that we can theorize and bound to our capacity to encode it.
 Furthermore, the comparative quality of these features can only be evaluated
 through their use in a supervised pattern recognition framework and not
 directly.
 Hence, the expressivity of these features is measured by their ability
 to provide an accurate prediction of data, in which case they are said
 to be robust.
\end_layout

\begin_layout Standard
However, this design scheme seems to be limited by our own experience and
 expectations on the shape of the optimization.
 However, this limits the potential exploration of the feature space and
 if our own priors on the data turns out to be only speculative, we might
 even be unwillingly constraining the final search space to only sub-optimal
 regions.
 Furthermore, features are usually repurposed from one problem to another
 for which they have clearly not be designed for, which might even worsen
 their sub-optimality.
\end_layout

\begin_layout Standard
However, feature extraction is typically constructed as an ordered combination
 of sets of (linear and non-linear) operations.
 Hence, instead of relying on manual search and optimization, this process
 could be reified through an automatic procedure of finding the optimal
 layering of operations.
 Therefore, feature design itself can be seen as a learning problem with
 its own search space, with the goal of finding the optimal transformation
 of input data that leads to the most expressive representation.
 Deep learning provides both a flexible and elegant solution to this problem
 by generalizing the feature and semantic optimization steps into the same
 framework.
\end_layout

\begin_layout Standard

\series bold
+ Idea of 
\emph on
distributed representation ?
\end_layout

\begin_layout Standard

\series bold
+ Pitfalls of 
\begin_inset Quotes eld
\end_inset

local template matching ?
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsection
Manifold and complexity
\end_layout

\begin_layout Standard
The feature learning paradigm is also driven by a strong underlying hypothesis
 that within the full space of possible datas (
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

 for 
\begin_inset Formula $n-$
\end_inset

dimensional vectors), the 
\emph on
real
\emph default
 data lives on a complex and highly non-linear manifold of lower dimensionality
 which occupies only a small portion of this full space.
 In this case, the input space is said to be 
\emph on
over-complete 
\emph default
with respect to the data.
 Hence, being able to accurately model this hypothetical manifold embedded
 within the input space would allow to disentangle the uninformative variance
 and provide the optimal data representation.
\end_layout

\begin_layout Standard
Under this assumption, the low-order, shallow architectures would be ill-equiped
 to accurately represent the complexity of this manifold accurately (as
 the complexity of data distribution would exceeds that of the model).
 Indeed, in order to cope with the large number of non-linear boundaries
 of this manifold, simple model would need to compensate with a very wide
 number of piece-wise approximations.
 In this sense, deep architectures can leverage the non-linear combinations
 to provide an exponentially increasing model complexity.
\end_layout

\begin_layout Standard
These ideas of targeting directly a model of this manifold are being increasingl
y studied, notably in the field of 
\emph on
representation learning
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2013representation"

\end_inset

.
 This is also one of the major properties of the generalization properties
 of deep architectures.
 The learning procedures are usually targeted at giving low reconstruction
 error on the data points coming from the same data-generating distribution
 as the training set, while having high reconstruction error on random sample
 of the full input space.
 Hence, deep architectures indirectly model this non-linear manifold.
 This idea has also been directly targeted through the notion of space embedding
 
\begin_inset CommandInset citation
LatexCommand cite
key "weston2012deep"

\end_inset

 or with special types of penalties 
\begin_inset CommandInset citation
LatexCommand cite
key "rifai2011contractive"

\end_inset

.
\end_layout

\begin_layout Subsection
Application to audio
\end_layout

\begin_layout Standard
Application of deep learning to audio have seen a flourishing literature
 in the recent years 
\begin_inset CommandInset citation
LatexCommand cite
key "yu2011deep"

\end_inset

.
 As put forward by 
\begin_inset CommandInset citation
LatexCommand cite
key "humphrey2013feature"

\end_inset

, the short-time nature of current audio analysis algorithm is inherently
 unable to encode musically meaningful structure at the track level.
\end_layout

\begin_layout Standard
Music is deep : 
\bar under

\begin_inset Quotes eld
\end_inset

is that music is composed: pitch and loudness combine over time to form
 chords, melodies and rhythms, which in turn are built into motives, phrases,
 sections and, eventually, construct entire pieces.
 This is the primary reason shallow architectures are ill-suited to represent
 high-level musical concepts and structure.
 A melody does not live at the same level of abstraction as a series of
 notes, but is instead a higher, emergent quality of those simpler musical
 objects.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Subsubsection
Time scales problem
\end_layout

\begin_layout Standard
Most music processing architectures are based on a short-time analysis paradigm.
 However, given the nature and construction of music, this approach is ill-suite
d to capture any form of syntagmatic and even paradigmatic higher-level
 knowledge.
 Music usually unfolds over both longer time-scales and that temporal informatio
n is usually fundamental to the perception of music.
 Hence, the bag-of-feature approach is inherently limited to descriptive
 static aspect frozen in time and the information transcribed is limited
 to the time scale of the analysis itself.
\end_layout

\begin_layout Standard
Furthermore, different temporal information coexist at various time scales
 which raise the question of temporal granularity.
 Even though, it appears quite complex to incorporate multiple and longer
 time scales, it is doubtful that any short-time analysis can encode any
 musically meaningful information.
\end_layout

\begin_layout Section
Architectures
\end_layout

\begin_layout Standard

\series bold
Globally need to talk about cost functions + gradient
\end_layout

\begin_layout Standard

\series bold
CF BENGIO + Representation learning article
\end_layout

\begin_layout Standard

\bar under
To implement this, one would start by manually deriving the expressions
 for the gradient of the loss with respect to the parameters: in this case
\bar default
 
\begin_inset Formula $\nicefrac{\partial\ell}{\partial\Theta_{i}}$
\end_inset


\end_layout

\begin_layout Subsection
Logistic regression
\end_layout

\begin_layout Standard
The logistic regression is the simplest form of probabilistic classifying
 network composed of a single layer.
 The underlying idea is that each class in a supervised problem can be represent
ed by an hyperplane separating the input space.
 Hence, by projecting an input vector onto each hyperplane, we can obtain
 the distance of this input to the hyperplane, which reflects its (inverse)
 probability to belong to the corresponding class.
 Formally, we wish to learn the parameters of the hyperplanes, defined by
 a weight matrix 
\begin_inset Formula $W$
\end_inset

 and a bias 
\begin_inset Formula $b$
\end_inset

.
 The probability that the input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 belongs to a particular class 
\begin_inset Formula $c$
\end_inset

 can be defined through the 
\emph on
softmax 
\emph default
operator
\begin_inset Formula 
\begin{eqnarray*}
P\left(Y=c\mid\mathbf{x},\mathbf{W},b\right) & = & softmax_{c}\left(\mathbf{W}\mathbf{x}+b\right)\\
 & = & \frac{e^{\mathbf{W}_{c}\mathbf{x}+b_{c}}}{\sum_{i}e^{\mathbf{W}_{i}\mathbf{x}+b_{i}}}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Then we can select the class which imply the highest probability as the
 prediction 
\begin_inset Formula $c_{pred}$
\end_inset

 such that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
c_{pred}=\underset{c}{argmax}\left[P\left(Y=c\mid\mathbf{x},W_{c},b_{c}\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
In order to obtain the best classification accuracy, the parameters 
\begin_inset Formula $\theta=\left\{ \mathbf{W},\mathbf{b}\right\} $
\end_inset

 of the hyperplanes are learned through the minimization of the loss function.
 In this case, we seek to reduce the number of misclassified example.
 Thus, this turns out to be equivalent to maximizing the log-likelihood
 of the dataset 
\begin_inset Formula $\mathcal{D}$
\end_inset

 under the parameters 
\begin_inset Formula $\theta$
\end_inset

 defined by
\emph on

\begin_inset Formula 
\begin{eqnarray*}
\mathcal{L}\left(\theta=\left\{ W,b\right\} ,\mathcal{D}\right) & = & \sum_{i=1}^{\left|\mathcal{D}\right|}log\left(P\left(Y=y^{(i)}\mid\mathbf{x}^{(i)},W,b\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Finally the complete cost function (over a set of 
\begin_inset Formula $n$
\end_inset

 training examples) used for multi-class logistic regression is defined
 through the softmax operator
\end_layout

\begin_layout Standard

\emph on
\begin_inset Formula 
\[
J(\theta)=-\frac{1}{n}\left[\sum_{i=1}^{n}\sum_{j=1}^{k}1\left\{ y^{(i)}=j\right\} \log\frac{e^{\theta_{j}^{T}x^{(i)}}}{\sum_{l=1}^{k}e^{\theta_{l}^{T}x^{(i)}}}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $1\{\cdot\}$
\end_inset

 is the indicator function.
 This leads to the gradient used for weights update
\end_layout

\begin_layout Standard

\emph on
\begin_inset Formula 
\[
\nabla_{\theta_{j}}J(\theta)=-\frac{1}{n}\sum_{i=1}^{n}\left[x^{(i)}\left(1\{y^{(i)}=j\}-p(y^{(i)}=j|x^{(i)};\theta)\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\nabla_{\theta_{j}}J(\theta)$
\end_inset

 defines the vector of derivatives of the cost with respect to the parameters
 of the model (the i-th element is the partial derivative of 
\begin_inset Formula $J(θ)$
\end_inset

 with respect to the i-th parameter).
\end_layout

\begin_layout Paragraph
Properties
\end_layout

\begin_layout Standard
One interesting property of the softmax classifier is that subtracting a
 particular value 
\begin_inset Formula $\alpha$
\end_inset

 from every parameter 
\begin_inset Formula $\theta_{i}$
\end_inset

 does not affect the prediction, which means that 
\begin_inset Formula $\mathcal{J}\left(\theta\right)=\mathcal{J}\left(\theta-\alpha\right)$
\end_inset

.
 Hence, if the cost function 
\begin_inset Formula $J(\theta)$
\end_inset

 is minimized by a set of parameters 
\begin_inset Formula $\theta=\left\{ \theta_{1},\ldots,\theta_{n}\right\} $
\end_inset

, then it is also minimized by 
\begin_inset Formula $\theta^{'}=\left\{ \theta_{1}-\psi,\ldots,\theta_{n}-\psi\right\} $
\end_inset

 for any value of 
\begin_inset Formula $\alpha$
\end_inset

, because the parameters of the softmax regression are redundant.
 Even though this shows that the minimizer of 
\begin_inset Formula $J(\theta)$
\end_inset

 is not unique, this property comes in handy to avoid a potential overflow
 in the parameters values.
\end_layout

\begin_layout Subsection
Neural networks
\end_layout

\begin_layout Subsubsection
Artificial neuron
\end_layout

\begin_layout Standard
The computational model of a neuron trace back to the seminal work of McCulloch
 and Pitts in 1943 .
 An artificial neuron is a computational unit that combines the weighted
 sum of its input and activates if this sum is over a threshold.
 Formally, a neuron is parametrized by a weight vector 
\begin_inset Formula $\mathbf{W}\in\mathbb{R}^{n}$
\end_inset

 and a bias 
\begin_inset Formula $b\in\mathbb{R}$
\end_inset

.
 For a given input vector 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{n}$
\end_inset

, the neuron outputs
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
{\textstyle h(\mathbf{x})=\phi(\mathbf{W}^{T}\mathbf{x}+b)=\phi(\sum_{i=1}^{n}W_{i}x_{i}+b)}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\phi:\mathbb{R}\mapsto\mathbb{R}$
\end_inset

 is called the 
\emph on
activation function.
 
\emph default
Hence, we can see that a neuron is a combination of an affine transform
 and a non-linearity that decides whether the neuron activates.
\end_layout

\begin_layout Subsubsection
Various computation units
\end_layout

\begin_layout Paragraph
Sigmoid
\end_layout

\begin_layout Standard
The most common activation function used in the literature (tracing back
 to the original works on neural networks) is the sigmoid function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi\left(z\right)=\frac{1}{1+e^{-z}}
\]

\end_inset


\end_layout

\begin_layout Standard
In this case, a single neuron is defined similarly to the mapping performed
 by logistic regression.
\end_layout

\begin_layout Paragraph
Hyperbolic tangent
\end_layout

\begin_layout Standard
Another usual choice for the activation function 
\begin_inset Formula $\phi$
\end_inset

 is the hyperbolic tangent (noted 
\emph on
tanh
\emph default
), defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\phi(z)=tanh\left(z\right)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Gaussian
\end_layout

\begin_layout Standard
Gaussian activation functions have been used in the form of 
\emph on
radial basis functions 
\emph default
(RBFs) in the so-called RBF networks and is defined as
\begin_inset Formula 
\[
\phi\left(\mathbf{x}\right)=exp\left(-\frac{\left\Vert \mathbf{x}-\mu\right\Vert ^{2}}{2\sigma^{2}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
These functions are more successful in the case where neural networks are
 used as function approximators.
\end_layout

\begin_layout Paragraph
Noisy Rectified linear
\end_layout

\begin_layout Standard
The original idea of the rectified linear units was to use N units to model
 a discrete variable that could take N different values.
 The N units share the same weights and biases, and the outputted number
 was simply the number of units being on.
 The inappropriate behavior of those units for extreme values of the activation
 probability 
\begin_inset Formula $p$
\end_inset

lead to another representation Stepped Sigmoid Units (SSU) which still used
 N units, but with a regularly increasing bias 
\begin_inset CommandInset citation
LatexCommand cite
key "nair2010rectified"

\end_inset

.
 However, this method require an increasing number of sampling when N increase.
 A good yet simple approximation of the N unit is given by the Noisy Rectified
 Linear Unit (NReLU) whose probability distribution is directly given by
 
\begin_inset Formula 
\[
\phi(x)=\max(0,x+\mathcal{N}(0,\sigma(x)))
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathcal{N}(0,v)$
\end_inset

 is a Gaussian noise of variance 
\begin_inset Formula $v$
\end_inset

 and 
\begin_inset Formula $x=W.h+b_{v}$
\end_inset

 is the input of the visible unit.
\end_layout

\begin_layout Standard
Just some notes about why does ReLU work : how to model a discrete variable
 with N states ? With N binary unit, the output is equal to 
\begin_inset Formula $n\in\{0,N-1\}$
\end_inset

 where 
\begin_inset Formula $n$
\end_inset

 is the number of units being on.
 Hence, the mean value is 
\begin_inset Formula $Np$
\end_inset

 and variance 
\begin_inset Formula $Np(p-1)$
\end_inset

.
 For a small value of 
\begin_inset Formula $p$
\end_inset

, this is equivalent to a Poisson unit.
 The problem is that the variance is exponential in 
\begin_inset Formula $p$
\end_inset

 and becomes very close to 0 for 
\begin_inset Formula $p$
\end_inset

 close to 1.
 A solution is to use Stepped Sigmoid Units (SSU) which is a sum of sigmoids
 units which share the same weights but with a constantly increasing bias.
 The sum of their probability is approximately equal to 
\begin_inset Formula $\log(1+\exp(x))$
\end_inset

 (D = sum SSU
\begin_inset Formula $\approx\text{integral diff(log(1+exp(x)))}$
\end_inset

).
 
\begin_inset Formula $\phi$
\end_inset

 is then a good approximation of this function.
\end_layout

\begin_layout Paragraph
Others
\end_layout

\begin_layout Standard
Several other types of computational units have been proposed such as
\end_layout

\begin_layout Itemize
Multiquadratics and inverse multiquadratics
\end_layout

\begin_layout Itemize
Binary and bipolar step (Heaviside) function
\end_layout

\begin_layout Itemize
Ramp and identity function
\end_layout

\begin_layout Subsubsection
Multi-layer neural networks
\end_layout

\begin_layout Standard
It seems clear than from the definition of a single artificial neuron, we
 could construct a whole network based on this unit.
 In this network the output of a neuron could 
\begin_inset Quotes eld
\end_inset

feed
\begin_inset Quotes erd
\end_inset

 the input of another one.
 Hence the multi-layer neural network (often called 
\emph on
multi-layer perceptron 
\emph default
(MLP)) are constructed by setting layers of neuron one over the other.
 Each layer is densely connected to the next layer, with the output of one
 neuron connected to the inputs of all neurons of the next layer.
 If we feed an input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to the network, the neurons with a sufficiently strong weighted input signal
 will activate and emit an activation.
 The activations of this layer will in turn become the input of the next
 layer which will perform the same computations.
 Hence, given the activation vector 
\begin_inset Formula $\mathbf{a}^{(l)}$
\end_inset

 of a given layer 
\begin_inset Formula $l$
\end_inset

, we can compute the activation of the next layer 
\begin_inset Formula $\mathbf{a}^{(l+1)}$
\end_inset

 as
\emph on
 
\begin_inset Formula 
\[
a^{(l+1)}=\phi(\mathbf{W}^{(l)}\mathbf{a}^{(l)}+b^{(l)})
\]

\end_inset


\end_layout

\begin_layout Standard
by setting 
\begin_inset Formula $\mathbf{a}^{(0)}=\mathbf{x}$
\end_inset

 to be the input.
 This step is called the 
\emph on
forward propagation
\emph default
, as the activations are propagated from one layer to the next.
 As the network is considered to be densely connected, we can organize its
 parameters in matrices and use matrix-vector operations to take advantage
 of fast linear algebra routines to perform quick calculations.
\end_layout

\begin_layout Subsubsection
Learning algorithm
\end_layout

\begin_layout Standard
If the goal of the network is to output a known (desired) vector of values
 
\begin_inset Formula $\mathbf{y}$
\end_inset

, we can compute the amount of error in the approximation made by the network.
 This will subsequently allow the network to learn from its own mistakes
 (by comparing the output of its last layer 
\begin_inset Formula $h\left(\mathbf{x}\right)$
\end_inset

 to the desired one 
\begin_inset Formula $\mathbf{y}$
\end_inset

).
 This function, called the 
\emph on
cost function
\emph default
 can be defined for a single input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{J}(W,b;\mathbf{x},\mathbf{y})=\left\Vert h(\mathbf{x})-\mathbf{y}\right\Vert ^{2}
\]

\end_inset

This squared-error cost function simply evaluate the Euclidean distance
 between the output produced by the network and the desired output 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
 Given the entire set of 
\begin_inset Formula $n$
\end_inset

 training examples, we can define the global cost function as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{J}\left(W,b\right)=\left[\frac{1}{n}\sum_{i=1}^{n}\left(\left\Vert h(\mathbf{x}^{(i)})-\mathbf{y}^{(i)}\right\Vert ^{2}\right)\right]+\lambda\sum_{i,j,l}\left(W_{ji}^{(l)}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The first term simply defines the (squared) 
\emph on
prediction error 
\emph default
of the network.
 The second term is used to bound the magnitude of the weights which is
 intended to prevent 
\emph on
overfitting
\emph default
 (as unbounded weights can also increase linearly in the same direction
 over learning).
 The term (often called the 
\emph on
weight decay penalty
\emph default
) acts as a regularization and its impact is controled by the 
\emph on
weight parameter 
\emph default

\begin_inset Formula $\lambda$
\end_inset

.
\end_layout

\begin_layout Standard
In order to learn in this network, we need to first initialize the parameters
 randomly and then evaluate the error of the network.
 Then, by looking at the gradient of the error with respect to each parameter
 (how much will the error increase or decrease depending on how we change
 this parameter), we can find the best direction to take.
\end_layout

\begin_layout Standard
It is important to note that the parameters 
\emph on
must be
\emph default
 initialized with small random values instead of all zeros.
 Indeed, if all the parameters are set to the same values, then all neurons
 would provide the same output and, thus, the error gradients would be similar
 for all units, ending in an array of units learning the exact same function.
 Therefore, this random initialization allows to enforce a 
\emph on
symmetry breaking
\emph default
.
\end_layout

\begin_layout Standard
The derivative of a complete network can be a complex task.
 Fortunately, the backpropagation algorithm was developped to simplify the
 updates of a function network, by seeing that the gradients are usually
 functions of the gradients of deeper layers.
 The complete algorithm is as follows
\end_layout

\begin_layout Enumerate
Perform a forward propagation in the network in order to obtain the activations
 
\begin_inset Formula $a^{(l)}$
\end_inset

 for each layer up to the last (output) layer 
\begin_inset Formula $a^{(n_{l})}$
\end_inset


\end_layout

\begin_layout Enumerate
In the case of a squared error, the output layer partial derivative is given
 by 
\begin_inset Formula 
\begin{align*}
\delta^{(n_{l})}=-2(y-a^{(n_{l})})\bullet\phi'(z^{(n_{l})})
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
For all previous layers, we can compute the partial derivatives 
\begin_inset Formula 
\begin{align*}
\delta^{(l)}=\left((W^{(l)})^{T}\delta^{(l+1)}\right)\bullet\phi'(z^{(l)})
\end{align*}

\end_inset


\end_layout

\begin_layout Enumerate
The final derivatives with respect to different parameters is given by 
\begin_inset Formula 
\begin{align*}
\nabla_{W^{(l)}}\mathcal{J}(W,b;x,y) & =\delta^{(l+1)}(a^{(l)})^{T}\\
\nabla_{b^{(l)}}\mathcal{J}(W,b;x,y) & =\delta^{(l+1)}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsubsection
Gradient descent
\end_layout

\begin_layout Standard
In order to update the parameter, we need to take a step in the direction
 opposite to the gradient.
 Indeed, if the gradient is decreasing, the error is decreasing in the direction
 of increasing the parameter value.
 Therefore, we can simply use the value of this gradient 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\theta_{i}^{(l)}=\theta_{i}^{(l)}-\epsilon\left(\frac{\partial\mathcal{J}(W,b;x,y)}{\partial\theta_{i}^{(l)}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\epsilon$
\end_inset

 is called the 
\emph on
learning rate 
\emph default
which controls the size of the step we will take at each iteration.
 Although it would seem natural to set this parameter to the maximal value,
 this increases the risk of overshooting (where the update makes steps so
 wide that it can 
\begin_inset Quotes eld
\end_inset

jump over
\begin_inset Quotes erd
\end_inset

 the optimal values).
 Oppositely, setting a low learning rate will cause the algorithm to be
 very slow to converge.
\end_layout

\begin_layout Standard
As the learning rate appears to be the most sensitive parameters of the
 learning, several variations have been devised such as 
\emph on
adadelta
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "duchi2011adaptive"

\end_inset

, which automatically tunes this parameter in order to quickly converge
 towards the local optimum.
 Other more sophisticated methods such as L-BFGS rely on quasi-Newton's
 method by trying to approximate the Hessian matrix to provide faster convergenc
e 
\begin_inset CommandInset citation
LatexCommand cite
key "liu1989limited"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Présenter ici les différentes méthode de gradient descent (AdaGrad, Hesian
 free, Newton...) dans Applying deep learning
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Difficulty of training deep architectures
\end_layout

\begin_layout Standard
The major obstacle in training and learning deep networks is the omnipresence
 of local optima in the objective function of the deep networks 
\begin_inset CommandInset citation
LatexCommand cite
key "yu2011deep"

\end_inset

.
 When trying to apply backpropagation to optimize the wide array of network
 parameters, these are usually initialized with randomly distributed points.
 Hence, depending on the position of this randomly chosen starting point,
 the subsequent local gradient descent can easily get trapped in a local
 optima and this pervasive problem increases significantly with the network
 depth, as it increases the number of parameters and number of local optima
 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2007greedy"

\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
Lack of availability of labeled data required for supervised training.
\end_layout

\begin_layout Itemize

\emph on
On the pervasive presence of local optima
\emph default
.
\end_layout

\begin_layout Itemize

\emph on
Diffusion of the error gradient
\emph default
.
 When performing the back-propogation algorithm, the error gradients are
 computed at the final layer and then propagated backwards.
 However, at each step towards a previous layer, these are multiplied by
 the derivative of the current layer (typically constrained by regularization
 to remain small).
 Therefore, the gradients and their impact on the weights will quickly diminish
 in magnitude as the depth of the network increases.
 Therefore, the impact of the derivative of the overall cost becomes less
 and less significant at each earlier layers, which greatly reduces the
 ability of these layers to learn.
\end_layout

\begin_layout Subsection
Unsupervised (self-taught) learning
\end_layout

\begin_layout Standard
It is well-known that in machine learning problems, best performance can
 be achieved simply by feeding more data to the algorithms, sometimes even
 overshadowing the strength of the algorithms themselves.
 To achieve this goal, the unsupervised feature learning (sometimes termed
 
\emph on
self-taught learning)
\emph default
 framework holds the promise that algorithms could learn from any unlabeled
 data.
 As the only constraint is that this data should be of the same 
\emph on
nature 
\emph default
as the studied problem, massive amounts of data can be easily obtained to
 perform learning.
 However, self-taught learning setting does not assume that the unlabeled
 data is drawn from the same data-generating distribution as the labeled
 data.
 The only requirement is that dimensions are of the same nature.
 This would allow to learn the underlying structure of the data from a wide
 array of unlabeled examples and then fine-tuning the learning from a smaller
 amount of labeled training data to target a specific supervised task.
 Fine-tuning significantly improves the classifying performance by exploiting
 the labeled training set to more closely fit to the statistical regularities
 of a specific problem.
\end_layout

\begin_layout Standard
Based on a learned network of abstractions, feeding an input vector to it
 provides a vector of activations in the last layer.
 These activations are supposedly a higher-level and more efficient representati
on of the input.
 Then, we can either just 
\emph on
replace
\emph default
 the original vector with the activation vector or 
\emph on
concatenate
\emph default
 the two feature vectors (this can also be seen as a network where both
 the activation and the input directly feed directly the classifying layer).
 However, it has been shown that this concatenation representation provides
 only marginal changes to the replacement operation 
\series bold
[xREFx]
\end_layout

\begin_layout Subsection
Greedy layer-wise training
\end_layout

\begin_layout Standard

\series bold
\bar under
MERGE WITH UPWARDS
\series default
 
\series bold
OR MAYBE TRASH
\series default
 
\series bold
(BIS REPETITA) 
\series default
One method that has seen some success is the greedy layer-wise training
 method.
 Training can either be supervised (with classification error on each step),
 but more frequently it is unsupervised.
 This method trains the parameters of each layer individually while freezing
 parameters for the remainder of the model.
 The weights from training the layers individually are then used to initialize
 the weights in the final/overall deep network, and only then is the entire
 architecture "fine-tuned" (i.e., trained together to optimize the labeled
 training set error).
\end_layout

\begin_layout Itemize

\emph on
Bypassing the lack of labeled data
\emph default
.
 As the self-taught and greedy layer-wise learning approaches are able to
 exploit massive amounts of unlabeled data (given the only requirement that
 these data are of same nature, i.e.
 pertain to the same underlying data-generating distribution), these methods
 bypass the need for large collections of labeled data.
 Furthermore, using unlabeled provides strongly better initial values for
 the weights in all pretraining layers (except the final classification
 layer that requires labeled data for training).
 Hence, these algorithms can uncover more robust patterns by thriving on
 massively more amounts of data than supervised approaches.
\end_layout

\begin_layout Itemize

\emph on
Better local optima
\emph default
.
 By starting at a more advantageous region of the parameter space than with
 random initialization, fine-tuned networks starting from this more optimal
 location can lead to better local optima.
 Intuitively, gradient descent from a pre-trained location embeds a significant
 amount of "prior" knowledge extracted from the underlying structure of
 unlabeled data.
\end_layout

\begin_layout Itemize

\emph on
Providing a logical decomposition of a task into a set of sub-problems at
 different levels of abstraction
\end_layout

\begin_layout Standard
It has been shown that this unsupervised pre-training approach provides
 a strong data-driven prior 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

, which can be seen as a form of regularization 
\begin_inset CommandInset citation
LatexCommand cite
key "erhan2009difficulty"

\end_inset

.
 Indeed, by exploiting the structure of the nature of the data itself (independe
ntly of the eventual supervised task at hand), the feature learning layers
 are initialized in a more advantageous region of the parameter space, which
 provides better local optima and requires less labeled data for converging
 to an adequate network.
\end_layout

\begin_layout Subsection
Reconstruction goal
\end_layout

\begin_layout Standard
Even though the general idea of layer-wise training appears as an ideal
 way to handle deep architectures, its pragmatic learning goal (or objective
 function) remains to be defined.
 We know that we hope to learn at each layer a higher-level abstraction
 based on statistical regularities of the previous lower levels.
 So the guiding principle for learning these intermediate representations
 should be to capture the most relevant information of the underlying distributi
on of lower-level concepts.
 In order to define this in a pragmatic learning function, the key lies
 in the concept of 
\emph on
reconstruction
\emph default
.
 A concept from a higher layer should be constructed out of a set of concepts
 from the lower layers.
 So if we are able to decompose a specific abstraction in a limited set
 of pieces and then can reconstruct it back from this set, it means that
 we have discovered its most salient components.
 Formally, the objective function is to learn an encoding function 
\begin_inset Formula $e$
\end_inset

 that decompose the input and a decoding function 
\begin_inset Formula $d$
\end_inset

 that reconstruct this input from its code.
 These functions should minimize the error between an input 
\begin_inset Formula $x$
\end_inset

 and its version 
\begin_inset Formula $\tilde{x}$
\end_inset

 reconstructed from these function.
\begin_inset Formula 
\begin{eqnarray*}
Y & = & e_{\theta_{e}}\left(X\right)\\
\tilde{X} & = & d_{\theta_{d}}\left(Y\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
As can already be seen, an amount of regularization will be necessary to
 prevent the algorithm from learning the identity function.
 Furthermore, we would also like to ensure that the representation learned
 by the system remains robust to small variabilities in the input (notably
 with respect to the 
\emph on
manifold 
\emph default
hypothesis).
\end_layout

\begin_layout Section
Single-layer modules
\end_layout

\begin_layout Standard
Although targeted at learning deep architectures, there is a broad division
 in research depending on the interpretation given to the connexionnist
 architecture.
 First, the probabilistic view lead to models driven by probabilistic graphical
 models, interpreting the hidden units as latent random variables.
 Second, following the research on neural networks lead to model constructed
 through computation graphs, where hidden units are considered as computational
 nodes.
 These two views are not entirely dichotomic as their similarities seem
 to outweigh their differences.
 Indeed, it has been shown that these two views are in fact almost equivalent
 under certain assumptions 
\begin_inset CommandInset citation
LatexCommand cite
key "vincent2011connection"

\end_inset

.
\end_layout

\begin_layout Subsection
Auto-encoders
\end_layout

\begin_layout Standard
The Auto-Encoder (AE) was first introduced 
\begin_inset CommandInset citation
LatexCommand cite
key "rumelhart1988learning"

\end_inset


\series bold
 
\series default
as a dimensionnality reduction technique.
 In its original formulation, an AE is composed of an encoder and a decoder,
 where the output of the encoder provides a reduced (compressed) representation
 of the input and the decoder allows to reconstruct the orginal input from
 this encoded representation.
 Hence, both the encoding and decoding part are tuned through the minimization
 of a reconstruction error function, which finds a non-linear dimensionality
 reduction and representation fit to a certain dataset (as the encoder has
 a lower number of units than the input data).
 
\end_layout

\begin_layout Standard
We can see that the framework defined by AEs fit the overarching goal of
 unsupervised and self-taught learning, as it tries to exploit statistical
 correlations of the data structure to find a non-linear representation
 aimed at decomposing and then reconstructing the input.
 However, when trying to learn increasingly higher-level abstractions, it
 seems more logical to have an increasing explanatory power through higher
 dimensionnalities.
 Hence, as opposed to their historical dimensionality-reduction objectives,
 current auto-encoders are defined as 
\emph on
over-complete
\emph default
 (with an encoder layer of higher dimensionality than the input).
 This definition is aimed at extracting a set of features larger than the
 input.
\end_layout

\begin_layout Standard
It has been shown that when a linear activation function is used in the
 encoding layer and that it forms a bottleneck by having a number of units
 inferior to the input dimensionality, the learnt parameters of the encoder
 are a subspace of the input space principal components 
\begin_inset CommandInset citation
LatexCommand cite
key "baldi1989neural"

\end_inset

.
 
\series bold
\bar under
MAYBE TRASH/MERGE
\series default
This is similar to the way the projection on principal components would
 capture the main factors of variation in the data.
 Indeed, if there is one linear hidden layer (the code) and the mean squared
 error criterion is used to train the network, then the k hidden units learn
 to project the input in the span of the first k principal components of
 the data
\series bold
 MAYBE TRASH/MERGE
\series default
.
 
\bar default
However, the use of non-linear activation functions in the encoder provides
 a more expressive framework and lead to more useful feature-detectors than
 what can be obtained with a simple PCA as it provides a non-linear transformati
on of the input data.
\end_layout

\begin_layout Subsubsection
Basic auto-encoder
\end_layout

\begin_layout Standard
The autoencoder aims at learning both an encoding function 
\begin_inset Formula $e$
\end_inset

 and a decoding function 
\begin_inset Formula $d$
\end_inset

 such that 
\begin_inset Formula ${\textstyle d\left(e\left(\mathbf{x}\right)\right)=\tilde{\mathbf{x}}\approx\mathbf{x}}$
\end_inset

.
 Therefore, the AE is intended to learn a function approximating the identity
 function, by being able to reconstruct an 
\begin_inset Formula ${\textstyle \tilde{\mathbf{x}}}$
\end_inset

 similar to the input 
\begin_inset Formula $\mathbf{{\textstyle x}}$
\end_inset

 via a hidden representation.
 In the case of entirely random input data (for instance a set of IID Gaussian
 noise), this task would not only be hard but also quite meaningless.
 However, based on the assumption that there might exist an underlying hidden
 structure in the data (where part of the input features are consistently
 correlated), then this approach might be able to uncover and exploit these
 statistical regularities.
 The encoding function 
\begin_inset Formula $e:\mathbb{R}^{d_{x}}\rightarrow\mathbb{R}^{d_{h}}$
\end_inset

 maps an input 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{d_{x}}$
\end_inset

 to an hidden representation 
\begin_inset Formula $\mathbf{h}_{\mathbf{x}}\in\mathbb{R}^{d_{h}}$
\end_inset

 by producing a deterministic mapping 
\begin_inset Formula 
\[
\mathbf{h}_{\mathbf{x}}=e\left(\mathbf{x}\right)=s_{e}\left(\mathbf{W_{e}}\mathbf{x}+\mathbf{b}_{e}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $s_{e}$
\end_inset

 is a nonlinear activation function (usually the 
\emph on
sigmoid 
\emph default
function), 
\begin_inset Formula $\mathbf{W}_{e}$
\end_inset

 is a 
\begin_inset Formula $d_{h}×d_{x}$
\end_inset

 weight matrix , and 
\begin_inset Formula $\mathbf{b}_{e}\in\mathbb{R}^{d_{h}}$
\end_inset

 is a bias vector.
\end_layout

\begin_layout Standard
The decoding function 
\begin_inset Formula $d:\mathbb{R}^{d_{h}}\rightarrow\mathbb{R}^{d_{x}}$
\end_inset

 then maps back this encoded representation 
\begin_inset Formula $\mathbf{h_{x}}$
\end_inset

 into a reconstruction 
\begin_inset Formula $\mathbf{y}$
\end_inset

 of the same dimensionnality as 
\begin_inset Formula $\mathbf{x}$
\end_inset

 
\begin_inset Formula 
\[
\mathbf{y}=d\left(\mathbf{h_{x}}\right)=s_{d}\left(\mathbf{W}_{d}\mathbf{h_{x}}+\mathbf{b}_{d}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $s_{d}$
\end_inset

 is the activation function of the decoder.
 Usually the weight matrix of the decoding layer 
\begin_inset Formula $\mathbf{W}_{d}$
\end_inset

 is tied to be the transpose of the encoder weight matrix 
\begin_inset Formula $\mathbf{W}_{d}=\mathbf{W}_{e}^{T}$
\end_inset

, in which case the AE is said to have 
\emph on
tied weights
\emph default
.
\end_layout

\begin_layout Standard
Hence, training an auto-encoder can be summarized as finding the optimal
 set of parameters 
\begin_inset Formula $\theta=\left\{ \mathbf{W}_{e},\mathbf{W}_{d},\mathbf{b}_{e},\mathbf{b}_{d}\right\} $
\end_inset

 (or 
\begin_inset Formula $\theta=\left\{ \mathbf{W},\mathbf{b}\right\} $
\end_inset

 in the case of tied weights) in order to minimize the reconstruction error
 on a dataset of training examples 
\begin_inset Formula $\mathcal{D}_{n}$
\end_inset

 
\begin_inset Formula 
\[
\mathcal{J}_{AE}\left(\theta\right)=\sum_{\mathbf{x}\in\mathcal{D}_{n}}\mathcal{L}\left(\mathbf{x},d\left(e\left(\mathbf{x}\right)\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Usual choices for the reconstruction error function 
\begin_inset Formula $\mathcal{L}$
\end_inset

 are either the squared error 
\begin_inset Formula $\mathcal{L}(x,y)=\left\Vert x-y\right\Vert ^{2}$
\end_inset

 (often used for linear reconstruction) or the cross-entropy loss of the
 reconstruction 
\begin_inset Formula $\mathcal{L}(x,y)=−\sum_{i=1}^{d_{x}}x_{i}log\left(y_{i}\right)+\left(1-x_{i}\right)log\left(1-y_{i}\right)$
\end_inset

 (if the input is interpreted as vectors of probabilities and a sigmoid
 activation function is used).
 
\end_layout

\begin_layout Standard
As can be seen from the definition of the objective functions, by solely
 minimizing the reconstruction error, nothing prevents an auto-encoder with
 an input of 
\begin_inset Formula $n$
\end_inset

 dimensions and an encoding of the same (or higher) dimensionnality to simply
 learn the identity function.
 In this case, the AE would merely be mapping an input to a copy of itself.
 Surprisingly, it has been shown that non-linear autoencoders in this 
\emph on
over-complete
\emph default
 setting (with a hidden dimensionality strongly superior to that of the
 input) trained with stochastic gradient descent, could still provide useful
 representations, even without any additional constraints 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2007greedy"

\end_inset

.
 
\series bold
NON-LINEARITY ALREADY ACTS AS A REGULARIZER + 
\series default
\bar under
A simple explanation is that stochastic gradient descent with early stopping
 is similar to an L2 regularization of the parameters.
 To achieve perfect reconstruction of continuous inputs, an auto-encoder
 with non-linear hidden units needs very small weights in the first (encoding)
 layer, to bring the non-linearity of the hidden units into their linear
 regime, and very large weights in the second (decoding) layer.
 It means that the 
\emph on
representation is exploiting statistical regularities present in the training
 set
\emph default
, rather than merely learning to replicate the input.
\end_layout

\begin_layout Standard
Nonetheless, several penalties + 
\series bold
bla bla bla
\end_layout

\begin_layout Subsubsection
Regularized auto-encoders
\end_layout

\begin_layout Standard

\emph on
Weight-decay
\emph default
 is the simplest regularization technique targeted at preventing overfitting
 by favoring smaller weights in the learning.
 The idea is to add a penalty term on the magnitude of weights to the overall
 cost function 
\begin_inset Formula 
\[
\mathcal{J}_{AE}\left(\theta\right)=\left(\sum_{\mathbf{x}\in\mathcal{D}_{n}}\mathcal{L}\left(x,d\left(e\left(\mathbf{x}\right)\right)\right)\right)+\lambda\sum_{ij}W_{ij}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\lambda$
\end_inset

 is a parameter controlling the impact of the regularization on the global
 cost.
\end_layout

\begin_layout Subsubsection
Sparse auto-encoders
\end_layout

\begin_layout Standard
One solution to avoid the degeneracy and prevent the AE for learning the
 identity functio is to add a 
\emph on
sparsity
\emph default
 constraint to the cost function by forcing many of the hidden units to
 be zero or near-zero 
\begin_inset CommandInset citation
LatexCommand cite
key "ranzato2007sparse,lee2008sparse"

\end_inset

.
 Hence, by imposing this sparsity constraint, we can uncover interesting
 structures even when the number of hidden units is to the size of the input.
 The goal of finding this efficient representation would be to ensure that
 most hidden units are inactive for a large portion of the dataset (ie.
 features learned are 
\emph on
specific
\emph default
).
 For each hidden unit 
\begin_inset Formula $i$
\end_inset

, we can compute its average actiavtion (across the set of 
\begin_inset Formula $n$
\end_inset

 training examples) 
\begin_inset Formula 
\begin{align*}
\hat{\rho}_{i}=\frac{1}{n}\sum_{j=1}^{n}\left[a_{i}(\mathbf{x}_{j})\right]
\end{align*}

\end_inset

For the AE to be sparse, we would like the units to only activate on specific
 subsets, which is equivalent to enforcing the constraint for each hidden
 unit that 
\begin_inset Formula $\hat{\rho}_{i}=\rho$
\end_inset

 where 
\begin_inset Formula ${\textstyle \rho}$
\end_inset

 is called a sparsity target, typically close to zero.
 This would force the network to learn being able to reconstruct the input
 with a very limited number of units.
 To achieve this, we can add a penalty term to the overall optimization
 objective in order to penalize the mean activation of each unit 
\begin_inset Formula ${\textstyle \hat{\rho}_{i}}$
\end_inset

 that deviates significantly from the sought sparsity 
\begin_inset Formula ${\textstyle \rho}$
\end_inset

.
 This can be achieved by minimizing the Kullback-Leibler (KL) divergence
 between all 
\begin_inset Formula ${\textstyle \hat{\rho}_{i}}$
\end_inset

 and 
\begin_inset Formula $\rho$
\end_inset

, by considering the average activation and sparsity target as Bernoulli
 random variables
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum_{i}{\rm KL}(\rho,\hat{\rho}_{i})=\sum_{i}\rho log\frac{\rho}{\hat{\rho}_{i}}+(1-\rho)log\frac{1-\rho}{1-\hat{\rho}_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
Therefore, the complete cost function to minimize is defined by the sum
 of the normal cost function while accounting for this sparsity constraint
 defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{J}_{{\rm sparse}}\left(\theta\right)=\mathcal{J}\left(\theta\right)+\beta\sum_{i}{\rm KL}\left(\rho,\hat{\rho}_{i}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\beta$
\end_inset

 controls the influence of the sparsity constraint on the global cost, and
 the derivative of the constraint is given by
\begin_inset Formula 
\[
\delta_{sparse}=\beta\left(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
As we need to know the average activation value 
\begin_inset Formula ${\textstyle \hat{\rho}_{i}}$
\end_inset

 for each hidden unit, this penalty requires to compute a first forward
 pass on all the training dataset before computing the derivatives for each
 example.
 Overcomplete AEs with sparsity can be seen as an attempt to learn a series
 of traditionnal AE for each of the types of training data, which might
 also share some of their hidden structures.
\end_layout

\begin_layout Subsubsection
Denoising auto-encoders
\end_layout

\begin_layout Standard
Another very successful way to regularize AEs have been proposed by Vincent
 et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "vincent2010stacked"

\end_inset

, through the concept of Denoising Auto-Encoders (DAE).
 Intuitively, the main idea is to corrupt the input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 in order to produce
\series bold
 
\series default
a 
\begin_inset Quotes eld
\end_inset

noisy
\begin_inset Quotes erd
\end_inset

 version 
\begin_inset Formula $\tilde{\mathbf{x}}$
\end_inset

 before passing it to the AE, but then training the network with the goal
 to reconstruct the original (clean) version of 
\begin_inset Formula $\mathbf{x}$
\end_inset

 (producing an overall denoising process).
 Therefore, a DAE simultaneously tries to find a robust encoding of the
 input, while trying to remove the effect of a stochastic corruption process
 applied to its input in order to capture the relevant statistical dependencies
 in the input components.
 Training the autoencoder to reconstruct a clean input from a corrupted
 version of itself forces the hidden layer to uncover robust features but
 also inherently prevents it from learning the identity function.
 Hence, the objective function for DAE is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{J}_{DAE}\left(\theta\right)=\sum_{\mathbf{x}\in\mathcal{D}_{n}}\mathbb{E}_{\tilde{\mathbf{x}}\sim q\left(\mathbf{x}\mid\tilde{\mathbf{x}}\right)}\left[\mathcal{L}\left(\mathbf{x},d\left(e\left(\tilde{\mathbf{x}}\right)\right)\right)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where the corrupted versions 
\begin_inset Formula $\tilde{\mathbf{x}}$
\end_inset

 are obtained by applying a stochastic corruption process 
\begin_inset Formula $q\left(\tilde{\mathbf{x}}\mid\mathbf{x}\right)$
\end_inset

 to the input examples 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Any type of corruption process can be considered, typically including additive
 Gaussian noise and binary masking (a randomly selected subset of input
 components are set to 0).
 Variable amounts of corruption (variance of the Gaussian noise or number
 of dropped components) can be considered to control the degree of regularizatio
n.
 DAEs may be interpreted from a stochastic, information theoretic, generative
 or manifold learning, perspective 
\series bold
+ ADD REFERENCES FOR EACH OF THE PERSPECTIVE HERE
\end_layout

\begin_layout Standard

\series bold
+ ADD THE NEW 
\begin_inset Quotes eld
\end_inset

INFINITE DENOISE AE
\begin_inset Quotes erd
\end_inset

 HERE
\end_layout

\begin_layout Subsubsection
Contractive auto-encoders
\end_layout

\begin_layout Standard
Recently, another form of regularization called 
\emph on
contractive 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "rifai2011contractive"

\end_inset

 have been proposed to produce the contractive auto-encoders (CAE).
 The main idea behind CAE is to add a penalty term to the cost function
 based on the derivative of the hidden features with respect to the input.
 Hence, this encourages the learning to uncover features that have low variation
s in the local variations directions found directly in the data.
 This penalty term can be computed by taking the Frobenius norm of the Jacobian
 matrix of the non-linear encoding (hidden activations) with respect to
 the input.
 This penalty term might yield more robust features by creating contraction
 in the space localized around the training examples.
 This idea can be seen as a direct attempt to model the existence of a lower-dim
ensional non-linear manifold inside the complete input space.
\end_layout

\begin_layout Standard
Penalizing the norm of the Jacobian 
\begin_inset Formula $J_{e}\left(\mathbf{x}\right)$
\end_inset

 of the hidden mapping implicitly penalizes its 
\emph on
sensitivity
\emph default
 to a given input and encourages the robustness of the representation to
 slight variations in the input.
 Formally, given an input 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{d_{x}}$
\end_inset

 mapped to a hidden representation 
\begin_inset Formula $\mathbf{h}\in\mathbb{R}^{d_{h}}$
\end_inset

 (through an encoding function 
\begin_inset Formula $e$
\end_inset

), the contractive penalty term is given by the sum of the partial derivatives
 of the representation with respect to the input components
\begin_inset Formula 
\[
\left\Vert J_{e}\left(\mathbf{x}\right)\right\Vert _{F}^{2}=\sum_{ij}\left(\frac{\partial h_{j}\left(\mathbf{x}\right)}{\partial x_{i}}\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard

\bar under
Penalizing
\bar default
 
\begin_inset Formula $\left\Vert J_{e}\left(\mathbf{x}\right)\right\Vert _{F}^{2}$
\end_inset

 
\bar under
encourages the mapping to the feature space to be 
\emph on
contractive
\emph default
 in the neighborhood of the training data.
 The 
\emph on
flatness
\emph default
 induced by having low valued first derivatives will imply an 
\emph on
invariance
\emph default
 or 
\emph on
robustness
\emph default
 of the representation for small variations of the input.
\end_layout

\begin_layout Standard
The complete cost function of the CAE is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{J}_{AE}\left(\theta\right)=\sum_{\mathbf{x}\in\mathcal{D}_{n}}\left(\mathcal{L}\left(\mathbf{x},d\left(e\left(\mathbf{x}\right)\right)\right)+\lambda\left\Vert J_{f}\left(\mathbf{x}\right)\right\Vert _{F}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
While DAEs (indirectly) encourages the robustness of the reconstruction
 through corruption, CAEs tries to analytically encourage the robustness
 of representation itself by penalizing the magnitude of its variations
 in the neighborhood of training points.
 In the case of a sigmoid activation function, the contractive penalty can
 be simply calculated with
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left\Vert J_{e}\left(\mathbf{x}\right)\right\Vert _{F}^{2}=\sum_{i=1}^{d_{h}}\left(h_{i}\left(1-h_{i}\right)\right)^{2}\sum_{j=1}^{d_{x}}W_{ij}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
As we can see, in the case of a 
\emph on
linear 
\emph default
encoder (with an identity activation function), this penalty term is strictly
 equivalent to 
\begin_inset Formula $L^{2}$
\end_inset

 weight decay.
 One problem with the penalty introduced by CAE is that it might be limited
 to only 
\emph on
infinitesimal 
\emph default
variations in the input (because of the use of the first-order derivative).
 An extension to all higher-order derivatives has been proposed 
\begin_inset CommandInset citation
LatexCommand cite
key "rifai2011higher"

\end_inset

 leading to the CAE+H model.
\end_layout

\begin_layout Standard

\bar under
A high dimensional Jacobian contains directional information: the amount
 of contraction is generally not the same in all directions.
 The Frobenius norm measures the contraction of the mapping 
\emph on
locally
\emph default
 at that point.
 by the ratio of the distances between two points in their original (input)
 space and their distance once mapped in the feature space.
\end_layout

\begin_layout Subsubsection
Linear decoders
\end_layout

\begin_layout Standard
Because of the use of a sigmoid activation function in the decoding units
 of the autoencoders, the inputs are constrained to lie in the 
\begin_inset Formula $\left[0,1\right]$
\end_inset

 range (as the sigmoid only outputs numbers in that range).
 This can turn out to be problematic as most real-life data is not constrained
 to the unit range and there might not be a non-lossy way to scale the input
 data in this range.
 An easy way to alleviate this problem would be to simply remove the sigmoid
 function to obtain 
\begin_inset Formula $a=z$
\end_inset

.
 Formally, this can be achieved by replacing the sigmoid function in the
 output nodes by the identity function 
\begin_inset Formula $\phi(z)=z$
\end_inset

 (called in this case a 
\emph on
linear
\emph default
 activation function).
 The hidden layer of the AE, however, can still rely on a sigmoid activation
 function.
 An autoencoder with a sigmoid hidden layer and a linear output layer is
 called a 
\emph on
linear decoder
\emph default
.
 This type of autoencoder can be trained directly with real-valued inputs
 without the need to scale them to a specific range.
\end_layout

\begin_layout Standard
The only modification in the linear decoder is to replace the derivation
 in the last layer with
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\delta_{i}^{(3)}=-(y_{i}-\hat{x}_{i})
\]

\end_inset


\end_layout

\begin_layout Standard

\bar under
This will result in a model that is simpler to apply, and can also be more
 robust to variations in the parameters.
 
\series bold
(REF)
\end_layout

\begin_layout Subsection
Restricted Boltzmann Machine
\end_layout

\begin_layout Subsubsection
Energy function
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "yu2011deep"

\end_inset

:
\end_layout

\begin_layout Standard
A Restricted Boltzmann Machine (RBM) is a particular type of Markov random
 field composed of one layer of binary stochastic hidden units and another
 layer of stochastic visible (sometimes called 
\emph on
observable
\emph default
) units.
 Visible and hidden units are densely connected by undirected and weighted
 links.
 The joint probability distribution of the visible units 
\begin_inset Formula $v$
\end_inset

 and hidden units 
\begin_inset Formula $h$
\end_inset

, given the model parameters 
\begin_inset Formula $\theta$
\end_inset

 of an RBM 
\begin_inset Formula $p\left(v,h;\theta\right)$
\end_inset

 is defined through an energy function 
\begin_inset Formula $E\left(v,h;\theta\right)$
\end_inset

 such that
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(v,h;\theta\right)=\frac{e^{-E\left(v,h;\theta\right)}}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $Z=\sum_{v}\sum_{h}e^{-E\left(v,h;\theta\right)}$
\end_inset

 is a normalization factor called the 
\emph on
partition function
\emph default
 and the marginalized probability that the model assigns to a visible vector
 
\begin_inset Formula $v$
\end_inset

 is obtained by summing over all possible hidden vectors 
\begin_inset Formula 
\[
p\left(v;\theta\right)=\frac{\sum_{h}e^{-E\left(v,h;\theta\right)}}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
In the case where both the visible and hidden units are considered binary
 (Bernoulli) variables, the energy function of the joint configuration of
 
\begin_inset Formula $I$
\end_inset

 visible and 
\begin_inset Formula $J$
\end_inset

 hidden units 
\begin_inset Formula $(v,h)$
\end_inset

 is defined as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E\left(v,h;\theta\right)=\sum_{i=1}^{I}\sum_{j=1}^{J}w_{ij}v_{i}h_{j}-\sum_{i=1}^{I}b_{i}v_{i}-\sum_{j=1}^{J}a_{j}h_{j}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $w_{ij}$
\end_inset

 is the symmetric weight (called 
\emph on
interaction term
\emph default
) between visible unit 
\begin_inset Formula $v_{i}$
\end_inset

 and hidden unit 
\begin_inset Formula $h_{j}$
\end_inset

, and 
\begin_inset Formula $b_{i}$
\end_inset

, and 
\begin_inset Formula $a_{j}$
\end_inset

 are the bias terms of these respective units.
 The conditional probabilities that an hidden or visible unit is active
 is given by 
\begin_inset Formula 
\[
p\left(h_{j}=1\mid v;\theta\right)=\sigma\left(\sum_{i=1}^{I}w_{ij}v_{i}+a_{j}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(v_{i}=1\mid h;\theta\right)=\sigma\left(\sum_{j=1}^{J}w_{ij}h_{j}+b_{i}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\sigma\left(x\right)$
\end_inset

 is the sigmoïd function.
 It is often desirable (in typical real-life datasets) to consider the visible
 units as real-valued instead of binary by relying on Gaussian units.
 In this case, the RBM energy function is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E\left(v,h;\theta\right)=\sum_{i=1}^{I}\sum_{j=1}^{J}w_{ij}v_{i}h_{j}+\frac{1}{2}\sum_{i=1}^{I}\left(v_{i}-b_{i}\right)^{2}-\sum_{j=1}^{J}a_{j}h_{j}
\]

\end_inset


\end_layout

\begin_layout Standard
The conditional probabilities of the visible units being active become
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p\left(v_{i}\mid h;\theta\right)=\mathcal{N}\left(\sum_{j=1}^{J}w_{ij}h_{j}+b_{i},1\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Here the visible unit 
\begin_inset Formula $v_{i}$
\end_inset

 is considered real-valued by following a Gaussian distribution with mean
 
\begin_inset Formula $\sum_{j=1}^{J}w_{ij}h_{j}+b_{i}$
\end_inset

 and unit variance.
\end_layout

\begin_layout Subsubsection
Training procedure
\end_layout

\begin_layout Standard
The probability that the network assigns to an input training vector can
 be raised by lowering its energy, which amounts to adjusting the weights
 and biases of various units.
 Hence, the goal of learning is to lower the energy of an input while simultaneo
usly raising the energy of other data, especially those with low energies
 and which therefore make a large contribution to the partition function.
 In other words, the network tries to place high probabilities to the vectors
 that are part of the input dataset and low probabilities to the vectors
 that are not.
 The derivative of the log-likelihood 
\begin_inset Formula $\mbox{log }p\left(v;\theta\right)$
\end_inset

 of an input vector with respect to a weight is surprisingly simple.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial\mbox{log }p\left(v;\theta\right)}{\partial w_{ij}}=\mathbb{E}_{data}\left[v_{i}h_{j}\right]-\mathbb{E}_{model}\left[v_{i}h_{j}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\mathbb{E}_{data}\left[v_{i}h_{j}\right]$
\end_inset

 denotes the expectation under the distribution of the training data and
 
\begin_inset Formula $\mathbb{E}_{model}\left[v_{i}h_{j}\right]$
\end_inset

 is that same expectation under the distribution defined by the model.
 This leads to a very simple learning rule for performing gradient ascent
 over the log-probability of the training data.
 It is very easy to get an unbiased sample of the expectation under the
 distribution of the data 
\begin_inset Formula $\mathbb{E}_{data}\left[v_{i}h_{j}\right]$
\end_inset

 because there are no direct connections between hidden units in an RBM.
 Unfortunately, the expectation under the distribution of the model 
\begin_inset Formula $\mathbb{E}_{model}\left[v_{i}h_{j}\right]$
\end_inset

 is intractable to compute.
 However, Hinton et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

 proposed a Contrastive Divergence (CD) approximation to the gradient, where
 
\begin_inset Formula $\mathbb{E}_{model}\left[v_{i}h_{j}\right]$
\end_inset

 is replaced by running the Gibbs sampler initialized at the data for a
 given number of full steps.
 Interestingly, even though this Gibbs chain should be runned to infinity,
 it appears that a single full step is sufficient to obtain satisfactory
 results.
 A “reconstruction” is produced by setting each visible unit 
\begin_inset Formula $v_{i}$
\end_inset

 to 1 with the previously defined probability given by 
\begin_inset Formula $p\left(v_{i}=1\mid h;\theta\right)$
\end_inset

.
 The change in a weight is then given by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Delta w_{ij}=\epsilon\left(\mathbb{E}_{data}\left[v_{i}h_{j}\right]-\mathbb{E}_{recon}\left[v_{i}h_{j}\right]\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Regarding the update rule for biases, a simplified version of the learning
 rule can be used where pairwise products are replaced by the states of
 individual units instead.
 Even though these rules are approximations, the training seems to work
 surprisingly well even with a single step of CD.
 Interestingly, , have shown that these update rules do not correspond to
 the gradient of any function.
 Nevertheless, its success in many applications is also deeply tied to a
 careful selection of its parameters and implementation, such practical
 rules are provided in
\bar under
 
\bar default

\begin_inset CommandInset citation
LatexCommand cite
key "hinton2010practical"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Boltzmann Machine
\end_layout

\begin_layout Standard
Note that a more general model exists which is called Boltzmann Machine
 (BM).
 The energy function is defined by (omitting biases terms) :
\begin_inset Formula 
\[
E(v,h,\theta)=-\frac{1}{2}v^{\top}Lv-v^{\top}Wh-\frac{1}{2}h^{\top}Jh
\]

\end_inset

It can be seen in this expression that compared with the RBM, within layer
 connections are not removed.
 As for the RBM, the learning procedure proposed by Salakhutdinov 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2008learning"

\end_inset

 rely on updating the parameters by comparing
\emph on
 data-dependent expectations 
\emph default
and 
\emph on
model's expectation
\emph default
s of the different units.
 The major issue with this model is that exact inference can not be performed,
 since the hidden units are now dependent from each other.
 This make the inference of hidden units under data distribution much more
 difficult to obtain.
 
\end_layout

\begin_layout Standard
The 
\emph on
model's expectations 
\emph default
will be estimated through a Persistent Markov Chain, by running at each
 epoch of the algorithm a k-step Gibbs sampler thanks to the conditional
 distributions of the visible and hidden units.
 Note that a given hidden unit (respectively visible unit) depends on the
 visible units (resp.
 hidden) 
\emph on
and
\emph default
 the other hidden units (reps.
 visible).
 Hence, the Gibbs sampling process consists in randomly initializing 
\emph on
M fantasy particles 
\emph default
before the first epoch 
\emph on

\begin_inset Formula $\{\tilde{v}^{0,1},\tilde{h}^{0,1}\},...,\{\tilde{v}^{0,M},\tilde{h}^{0,M}\}$
\end_inset

 
\emph default
and run in parallel 
\emph on
M 
\emph default
Gibbs chains.
 After those k sampling steps, the value obtained is used to initialize
 the first terms of the Gibbs sampler for the next epoch 
\begin_inset Formula $\{\tilde{v}^{t+1,m},\tilde{h}^{t+1,m}\}_{init}=\{\tilde{v}^{t,m},\tilde{h}^{t,m}\}_{k}$
\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
data-dependent expectations 
\emph default
cannot be exactly inferred from the visible units, since they are conditionally
 dependent between each other.
 A method called variational learning is used, which roughly consists in
 approximating the true posterior distribution 
\begin_inset Formula $p(h|v;\theta)$
\end_inset

 by a for each training vector 
\begin_inset Formula $v$
\end_inset

 by an approximate distribution 
\begin_inset Formula $q(h|v;\mu)$
\end_inset

.
 This distribution is learned by trying to maximize a lower bound on the
 likelihood of the observed data with respect to the parameter 
\begin_inset Formula $\mu$
\end_inset

, the parameter 
\begin_inset Formula $\theta$
\end_inset

 being fixed.
 This gives a simple update rule for the variational parameters, given a
 vector 
\begin_inset Formula $v^{n}$
\end_inset

 from the training set 
\begin_inset Formula $\mu_{j}^{n}\leftarrow\sigma(\sum_{i}W_{ij}v_{i}^{n}+\sum_{m\backslash j}J_{mj}\mu_{m})$
\end_inset

 which can then be used as the mean-field value for each hidden units in
 the parameter update rule (here for W only) :
\begin_inset Formula 
\[
W^{t+1}=W^{t}+\alpha_{t}(\frac{1}{N}\sum_{n=1}^{N}v^{n}(\mu^{n})^{\top}-\frac{1}{M}\sum_{m=1}^{M}\tilde{v}^{t+1,m}(\tilde{h}^{t+1,m})^{\top})
\]

\end_inset

where 
\emph on
N
\emph default
 is the number of training example.
 After each epoch 
\emph on
t
\emph default
, the learning rate 
\begin_inset Formula $\alpha_{t}$
\end_inset

 must be decreased with time in order to guarantee the convergence of the
 algorithm.
 More details can be found in the reference article 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2008learning"

\end_inset

, but a recommended value is 
\begin_inset Formula $\alpha_{t}=\frac{1}{t}$
\end_inset

.
\end_layout

\begin_layout Standard
This complex fully connected structure will rarely be used, and restricted
 models would be preferred in most of the cases.
 However, it is good to keep in mind that this structure exists and that
 most of the model we will use derive from it.
 Besides, it is an interesting starting point when trying to build up a
 new model since it still offer a large panel of possibilities.
\end_layout

\begin_layout Subsubsection
Different types of unit
\end_layout

\begin_layout Standard
Even though the sigmoid has been historically the most used type of unit
 and that RBMs were designed for logistic binary units, other types of units
 can be set in the layers.
 The main goal of these other types of unit is to handle data which might
 be ill-suited to binary logistic visible units.
 Hence, the choice of unit should mainly depend on the nature of input data
 and its underlying distribution.
\end_layout

\begin_layout Standard

\series bold
+ Relate to activation function (which is considered as a probability in
 the case of RBMs)
\end_layout

\begin_layout Standard

\series bold
+ Warning when talking about different types of unit to well distinguish
 AE units (tanh / relu)
\end_layout

\begin_layout Paragraph
Sigmoid and softmax units
\end_layout

\begin_layout Standard
The probability of a sigmoïd unit to be active is given by the logistic
 sigmoid function of its input 
\begin_inset Formula 
\[
p=\sigma(x)=\frac{1}{1+e^{-x}}
\]

\end_inset


\end_layout

\begin_layout Standard
As we can see, if we generalize this function to take into account 
\begin_inset Formula $N$
\end_inset

 alternative states, we obtain the softmax unit 
\begin_inset Formula 
\[
p_{j}=\frac{e^{x_{j}}}{\sum_{i=1}^{N}e^{x_{i}}}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Gaussian units
\end_layout

\begin_layout Standard
For real-valued data such as the commonly studied inputs of most machine
 learning fields, binary units are a way too coarse approximation and can
 lead to a poor reconstruction.
 A simple solution to account for reals instead of binary numbers in the
 visible inputs is to introduce linear units with independent Gaussian noise.
 In the case of RBMs, the energy function becomes: 
\begin_inset Formula 
\[
E(v,h)=\sum_{i\in vis}\frac{\left(v_{i}-a_{i}\right)^{2}}{2\sigma_{i}^{2}}-\sum_{j\in hid}b_{j}h_{j}-\sum_{i,j}\frac{v_{i}}{\sigma_{i}}h_{j}w_{ij}
\]

\end_inset

with 
\begin_inset Formula $\sigma_{i}$
\end_inset

 is the standard deviation of the visible Gaussian unit 
\begin_inset Formula $i$
\end_inset

.
 It should be noted that the use of Gaussian units require a smaller learning
 rate, as now there is no bound on the magnitude of the output in the reconstruc
tion (oppositely, binary units being naturally bounded in value).
 Therefore, a component can become arbitrarily large which will result in
 a very large learning signal emanating from this single unit.
 Oppositely, the learning signal of binary units lie in the 
\begin_inset Formula $\left[-1,1\right]$
\end_inset

 range, which makes them more stable.
\end_layout

\begin_layout Paragraph
Binomial units
\end_layout

\begin_layout Standard
The simplest way to handle integer values between 
\begin_inset Formula $0$
\end_inset

 and 
\begin_inset Formula $N$
\end_inset

 is to rely on 
\begin_inset Formula $N$
\end_inset

 separate binary units but tying them to share identical weights and bias
 
\begin_inset CommandInset citation
LatexCommand cite
key "teh2001rate"

\end_inset

.
 As all these copies share the same parameters, the same input will result
 in the same activation probability that can be summed to obtain an integer.
 Furthermore, this probability can be computed only once for the whole set
 of copies.
 The utmost advantage of relying on these weight-sharing constructs to synthesiz
e a new type of unit is that the underlying model and mathematics of RBM
 remains unchanged.
\end_layout

\begin_layout Paragraph
Rectified linear units
\end_layout

\begin_layout Standard
By extending the previous reasoning to a potentially infinite number of
 copies, the sum of these shared probabilities tends to having a closed
 form: 
\begin_inset Formula 
\[
\sum_{i=1}^{\infty}\sigma\left(x-i+0.5\right)\approx\mbox{log}\left(1+e^{x}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $x=vw^{T}+b$
\end_inset

.
 It can be seen that this type of infinity of binomial units behaves like
 a smoothed rectified linear unit.
\end_layout

\begin_layout Standard

\bar under
Even though 
\begin_inset Formula $log(1+e^{x})$
\end_inset

 is not in the exponential family, we can model it accurately using a set
 of binary units with shared weights and fixed bias offsets.
 This set has no more parameters than an ordinary binary unit, but it provides
 a much more expressive variable.
\end_layout

\begin_layout Subsubsection
Conditional RBM
\end_layout

\begin_layout Standard
The first extension of the RBM proposed to handle multivariate time-series
 was the conditional RBM (cRBM) 
\begin_inset CommandInset citation
LatexCommand cite
key "taylor2006modeling,taylor2009factored"

\end_inset

.
 The cRBM is constructed with the same architecture as an RBM, but adding
 connections between a set of past visible vectors to the current hidden
 units.
 These links can be seen as vectors of auto-regressive weights that provide
 a form of short-term memory over the temporal structures.
 This dependency over a time frame of 
\begin_inset Formula $n$
\end_inset

 past visible units is modeled through the bias vectors of the cRBM defined
 as 
\begin_inset Formula 
\begin{eqnarray*}
b_{i}^{*} & = & b_{i}+\sum_{k=1}^{n}B_{k}\mathbf{x}_{\left(t-k\right)}\\
c_{i}^{*} & = & c_{i}+\sum_{k=1}^{n}A_{k}\mathbf{x}_{\left(t-k\right)}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $A_{k}$
\end_inset

 models the auto-regressive weights between a visible units at 
\begin_inset Formula $k$
\end_inset

 previous time steps and the current visible units and 
\begin_inset Formula $B_{k}$
\end_inset

 models the same relationship of the past visible but to the current hidden
 units.
 The order of the model is defined by the length of its memory, i.e.
 the number of previous time frames 
\begin_inset Formula $n$
\end_inset

 that are taken into account.
 The activation probabilities of the hidden and visible units are given
 respectively by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P\left(h_{j}=1\mid\mathbf{x}\right) & = & \sigma\left(b_{j}+\sum_{i}W_{ij}x_{i}+\sum_{k}\sum_{i}B_{ijk}x_{i}\left(t-k\right)\right)\\
P\left(x_{i}=1\mid\mathbf{h}\right) & = & \sigma\left(c_{i}+\sum_{j}W_{ij}h_{j}+\sum_{k}\sum_{i}A_{ijk}x_{i}\left(t-k\right)\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
The set of parameters 
\begin_inset Formula $\theta=\left\{ W,b,c,A,B\right\} $
\end_inset

 is trained like a traditionnal RBM through contrastive divergenceand the
 cRBM can also be used as a building block to create deeper networks.
 Conditional DBN has been used for human motion analysis [
\series bold
xREFx
\series default
].
 The conditional aspect associates the weight with a time window over the
 data from previous time steps.
 This leads to a type of temporal DBN improving the handling of temporal
 coherence aspects.
\end_layout

\begin_layout Subsubsection
Temporal RBM
\end_layout

\begin_layout Standard
A similar model is the Temporal RBM 
\begin_inset CommandInset citation
LatexCommand cite
key "sutskever2007learning"

\end_inset

 which has been proposed as an extension to cRBM.
 The idea is to provide context for the past visible units (same as the
 cRBM), but also to embed context information for the hidden states as well.
 Hence, the model is built of
\begin_inset Formula 
\[
P\left(\mathbf{h}_{t},\mathbf{x}_{t}\mid\mathbf{h}_{t-1},\mathbf{x}_{t-1},\ldots,\mathbf{h}_{t-k},\mathbf{x}_{t-k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where the context is defined for a window of 
\begin_inset Formula $k$
\end_inset

 time steps over both the visible and hidden units.
 Even if sampling in the Temporal RBM can be done in the same Markov chain
 approximation as in cRBMs, the inference aspect becomes intractable.
 This problem can be resolved by using a mean-field approximation instead
 of exact inference.
\end_layout

\begin_layout Subsubsection
Gated RBM
\end_layout

\begin_layout Standard
The Gated RBM (GRBM) 
\begin_inset CommandInset citation
LatexCommand cite
key "memisevic2007unsupervised"

\end_inset

 is another extension of the RBM targeted at modeling temporal data by directly
 incorporating the transitions between input vectors of consecutive time
 frames.
 To do so, the GRBM introduces a weight tensor 
\begin_inset Formula $W_{ijk}$
\end_inset

 which represent the interaction between the input 
\begin_inset Formula $x$
\end_inset

, the output 
\begin_inset Formula $y$
\end_inset

, and a set of latent variables 
\begin_inset Formula $z$
\end_inset

 (called 
\emph on
transformation variables
\emph default
) which are considered the hidden units.
 The energy function is defined as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E\left(y,z;x\right)=-\sum_{ijk}W_{ijk}x_{i}y_{i}z_{j}-\sum_{k}^{I}b_{k}z_{k}-\sum_{j}c_{j}y_{j}
\]

\end_inset


\end_layout

\begin_layout Standard
Hence, the GRBM tries to model directly what are the statistical regularities
 of going from one input to the next through the concept of transformation.
 The conditional probability of this transformation is given by
\begin_inset Formula 
\[
p\left(y,z\mid x\right)=\frac{e^{-E\left(y,z;x\right)}}{Z}
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\begin_inset Formula $Z$
\end_inset

 the partition function and the probability that hidden unit 
\begin_inset Formula $z_{i}$
\end_inset

 is active given an input 
\begin_inset Formula $x$
\end_inset

 and output 
\begin_inset Formula $y$
\end_inset

 is given by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
P\left(z_{k}=1\mid x,y\right) & = & \sigma\left(\sum_{ij}W_{ij}x_{i}y_{j}+b_{k}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
Each hidden variable 
\begin_inset Formula $z_{k}$
\end_inset

 learns an aspect of the transformation between the input 
\begin_inset Formula $x$
\end_inset

 and the output 
\begin_inset Formula $y$
\end_inset

.
 Hence, for a fixed input, the consecutive time frame creates a RBM learning
 the transformation that could produce this next output.
 This type of learning could not be achieved by simply concatenating two
 time frames and feeding it to a regular RBM since the latent variables
 would only thrive on the statistical regularities between that particular
 pair and not learning the general transformation.
\end_layout

\begin_layout Subsubsection
Factored RBM
\end_layout

\begin_layout Standard
The Factored RBM 
\begin_inset CommandInset citation
LatexCommand cite
key "mnih2007three"

\end_inset

 is a parametrization of RBM proposed to learn a distributed representation
 of words.
\end_layout

\begin_layout Section
Deep architectures
\end_layout

\begin_layout Subsection
Stacked auto-encoders
\end_layout

\begin_layout Standard
The DAE and RBM models are very shallow network in which a single layer
 of computation (given by the hidden layer activations) provides the set
 of learned features.
 However, the aim of deep neural networks is to obtain multiple hidden layers
 with each providing increasingly higher-level and complex features over
 the input by uncovering correlations through non-linear transformation
 of the previous layer.
 One of the key aspect in the expressive power of deep networks lie in the
 use of non-linear activation functions in various hidden layers.
 Indeed, as a combination of multiple applications of linear functions is
 itself only a linear function of the input, a network relying only on linear
 computations units could simply be reduced to a single layer of linear
 hidden units.
\end_layout

\begin_layout Standard
The representational power of deep networks is based on its ability to learn
 a structured hierarchy.
 This approach is based on the underlying hypothesis that higher-level concepts
 can be formed by grouping lower-level ones.
 Hence, each layer is expected to form increasingly complex features by
 learning how to optimially group concepts formed at the previous layer,
 by exploiting the statistical regularities of lower-level concepts.
\end_layout

\begin_layout Standard
Based on these observations, we see that we could form a deep network by
 simply stacking autoencoders on top of each other and feeding the latent
 representation of the auto-encoder at a specific layer as input to its
 above layer.
 In that case, the stacked autoencoder can either be interpreted as a list
 of separate autoencoders, or more globally as a traditional MLP.
 By relying on this type of dichotomy, we see that unsupervised pre-training
 can be done one layer at a time, by training each layer as an auto-encoder
 minimizing the reconstruction of the hidden representation output by the
 previous layer.
 After training the first 
\begin_inset Formula $k$
\end_inset

 layers, we can train the layer 
\begin_inset Formula $k+1$
\end_inset

 by computing the hidden representation obtained by iteratively passing
 the input data to all the 
\begin_inset Formula $k$
\end_inset

 layers below.
 After all the layers have been trained iteratively, the global view of
 the network allows to consider a second stage of training called 
\emph on
fine-tuning
\emph default
, usually by minimizing the error rate of a supervised task.
 In that case, a logistic regression (classification) layer can be added
 on top of the network in order to rely on the highest-level representation
 provided by the network (hidden activations of the last layer).
 The training procedure of the entire network is then exactly similar to
 that of a traditional MLP.
\end_layout

\begin_layout Standard
Following the idea of self-taught learning, features can be learned using
 only unlabeled data.
 However, it is also possible to combine, fine-tune and further improve
 these unsupervised features using labeled data.
 The overall classifying architecture can simply be considered as a deeper
 neural network.
\end_layout

\begin_layout Subsubsection
Pre-training stacked autoencoders
\end_layout

\begin_layout Standard
By relying on a greedy layerwise pretraining, a deep stacked autoencoder
 can be formed from a set of 
\begin_inset Formula $k$
\end_inset

 autoencoders, each parametrized by its own weight matrix 
\begin_inset Formula $\mathbf{W}_{e}^{(l)}$
\end_inset

 and biases 
\begin_inset Formula $b_{e}^{(l)}$
\end_inset

.
 Therefore, the output (activation) of the stacked autoencoder at each layer
 is produced by iteratively performing a forward pass of the encoding step
 from each layer to the next
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
a^{(l)} & = & \phi\left(\mathbf{W}_{e}^{(l-1)}\mathbf{a}^{(l-1)}+b_{e}^{(l-1)}\right)
\end{eqnarray*}

\end_inset

by considering that the first layer receives the output 
\begin_inset Formula $\mathbf{a}^{(0)}=\mathbf{x}$
\end_inset

.
 Conversely, the decoding step can be obtained by performing the decoding
 of each autoencoder in reverse order.
 The stacked auto-encoder ultimately produces a representation 
\begin_inset Formula $a^{(n)}$
\end_inset

, which is the activations of the deepest hidden units.
 This representation vector can be seen as a collection of highest-order
 features extracted from the input, which can in turn be used for classification
 problems with a softmax classifier.
\end_layout

\begin_layout Subsubsection
Fine-tuning stacked auto-encoders
\end_layout

\begin_layout Standard
As each layer of autoencoders is built on the output of the previous, the
 stacked autoencoder can be considered as a single model formed by a complete
 network.
 This global model could, therefore, be trained altogether by improving
 upon the weights of all layers at each iteration.
 This
\emph on
 finetuning
\emph default
 operation can be performed by discarding the decoding layers and replacing
 them by linking the last hidden layer to a softmax classifier.
 The error gradients from the supervised classification mistakes can then
 be backpropagated into all the encoding layers together.
\end_layout

\begin_layout Standard
As the backpropagation algorithm can be applied to a network of arbitrary
 depth, we can actually rely on the gradients from the final classification
 layer and backpropagate them across all the encoding layers.
 The only change to apply is to adapt the back-propagation step of the last
 layer to fit a softmax evaluation
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\delta^{(n_{l})}=-(\nabla_{a^{n_{l}}}J)\bullet f'(z^{(n_{l})})
\]

\end_inset


\end_layout

\begin_layout Standard

\bar under
(When using softmax regression, the softmax layer has
\bar default
 
\begin_inset Formula $\nabla J=\theta^{T}(I-P)$
\end_inset

 
\bar under
where 
\begin_inset Formula $I$
\end_inset

 is the input labels and 
\begin_inset Formula $P$
\end_inset

 is the vector of conditional probabilities.)
\end_layout

\begin_layout Subsection
Deep Belief Networks
\end_layout

\begin_layout Standard
Deep Belief Networks (DBNs) 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

 are a class generative models formed by multiple layers of stochastic computati
on units.
 The lowest layer (usually called the 
\emph on
visible layer
\emph default
) is composed of units, whose state (their latent variable) represent an
 input vector.
 The layers are connected with directed top-down connections from above
 layer and are called the 
\emph on
hidden units 
\emph default
(also called 
\emph on
feature detectors
\emph default
).
 The two upper layers are created with a dense array of undirected symmetric
 connections, which form an associative memory.
 Hence a 
\begin_inset Formula $l$
\end_inset

-layers DBN models the joint distribution between all hidden layers 
\begin_inset Formula $\mathbf{h}^{k}$
\end_inset

 and the observed data 
\begin_inset Formula $\mathbf{x}$
\end_inset


\begin_inset Formula 
\[
P\left(\mathbf{x},\mathbf{h}^{1},\ldots,\mathbf{h}^{k}\right)=\left(\prod_{k=0}^{(l-2)}P\left(\mathbf{h}^{k}\mid\mathbf{h}^{k+1}\right)\right)P\left(\mathbf{h}^{l-1},\mathbf{h}^{l}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\begin_inset Formula $\mathbf{h}^{0}=\mathbf{x}$
\end_inset

.
 We can see the construct of DBN through these two terms, where the lower
 layers are directed and the last one is formed as an RBM with undirected
 connections.
 We can see that stacking layers of RBMs (learned iteratively in a greedy
 layer-wise training) from the lowest (
\emph on
visible 
\emph default
data) to the upper (
\emph on
associative memory
\emph default
) layer can provide a DBN architecture.
 In that case, we consider that the activation probabilities of the hidden
 layer of one RBM becomes the visible data for the next RBM layer.
 It has been shown that relying on this stacking procedure improves the
 variational lower bound of the log-likelihood of the data 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

.
 This means that learning a DBN with this procedure provides a close approximati
on of the true maximum likelihood (ML) learning.
 
\end_layout

\begin_layout Standard
This pre-training and learning phase of the DBN is usually followed by the
 addition of a discriminative layer in order to perform discriminative task.
 This allows to 
\emph on
fine-tune 
\emph default
the whole networks by adjusting the complete sets of weights jointly in
 order to further improve the accuracy of the model.
 The advantages of the DBN is that it can be interpreted in the Bayesian
 framework as a probabilistic generative model, which allows both to efficiently
 compute the hidden variables, but also to sample from the network to synthesize
 new data 
\begin_inset CommandInset citation
LatexCommand cite
key "yu2011deep"

\end_inset

.
\end_layout

\begin_layout Standard
Tang and Eliasmith showed that sparse connection patterns in the first layer
 of the DBN and a probabilistic denoising algorithm could improve the robustness
 of the DBN 
\begin_inset CommandInset citation
LatexCommand cite
key "tang2010deep"

\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Extend this + FIGURE
\end_layout

\begin_layout Subsubsection
Evaluating DBN's
\end_layout

\begin_layout Standard
DBN's can be evaluated by a Monte Carlo based technic called Annealed Importance
 Sampling (AIS).
 AIS is a technic to evaluate the partition function of a complex probability
 distribution by approximating this distribution with a much simpler one
 for which the partition function is known.
 Knowing the partition function is a requirement to compute the log-likelihood
 over a test set 
\begin_inset Formula $\ln(p(v_{test};\theta))$
\end_inset

, which is often the best way to evaluate the performances of a model.
 We shortly introduce AIS in its most general context before applying it
 to the DBN model.
 Those two sections are largely inspired by the work of R.Salakhutdinov
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2008quantitative"

\end_inset

.
\end_layout

\begin_layout Paragraph
Annealed Importance Sampling (AIS)
\end_layout

\begin_layout Standard
Suppose that we have two distributions defined on the same space 
\begin_inset Formula $\mathcal{X}$
\end_inset

.
 Their probability density functions are given by the ratio of an unnormalized
 probability and a partition function : 
\begin_inset Formula $p_{A}=\frac{p_{A}^{*}(x)}{Z_{A}}$
\end_inset

 and 
\begin_inset Formula $p_{B}(x)=\frac{p_{_{B}^{*}(x)}}{Z_{B}}$
\end_inset

.
 Typically, 
\begin_inset Formula $p_{A}$
\end_inset

 is defined as a simple distribution for which the partition function 
\begin_inset Formula $Z_{A}$
\end_inset

 is known.
 
\begin_inset Formula $p_{B}$
\end_inset

 is an unknown complex distribution for which the partition function is
 intractable.
 AIS estimates the ratio of the two partition functions 
\begin_inset Formula $\frac{Z_{A}}{Z_{B}}$
\end_inset

, from which 
\begin_inset Formula $Z_{B}$
\end_inset

 is easy to obtain.
 Importance Sampling method consists in directly evaluating this ratio by
 sampling from the distribution equal to the ration of those two functions.
 A major issue with Importance Sampling is that if the distribution 
\begin_inset Formula $p_{A}$
\end_inset

 and 
\begin_inset Formula $p_{B}$
\end_inset

 are not close enough, the results will be very poor (the variance of the
 estimator ( 
\begin_inset Formula $\mathbb{V}[\text{\hat{r}}_{IS}]$
\end_inset

 ) will be very large in high dimension spaces).
 Annealed importance sampling addresses this issue by defining a sequence
 of K intermediate distributions 
\begin_inset Formula 
\[
p_{A}=p_{0},...,p_{K}=p_{B}
\]

\end_inset

Those distribution must respect the following rules
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\forall x\in\mathcal{X}$
\end_inset

, 
\begin_inset Formula $p_{k}(x)\neq0$
\end_inset

 if 
\begin_inset Formula $p_{k+1}(x)\neq0$
\end_inset


\end_layout

\begin_layout Enumerate
For all 
\begin_inset Formula $k\in\llbracket0,K\rrbracket$
\end_inset

, the intermediate unnormalized probabilities 
\begin_inset Formula $p_{k}^{*}(x)$
\end_inset

, 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 must be easily evaluated
\end_layout

\begin_layout Enumerate
For each 
\begin_inset Formula $k\in\llbracket0,K-1\rrbracket$
\end_inset

, a Markov chain transition operator must be defined such that a sample
 
\begin_inset Formula $x'$
\end_inset

 can be drawn given a sample 
\begin_inset Formula $x$
\end_inset

 
\begin_inset Formula 
\[
p_{k}(x')=\int T_{k}(x',x)p_{k}(x)dx
\]

\end_inset


\end_layout

\begin_layout Enumerate
Samples can be independently drawn from the distribution 
\begin_inset Formula $p_{A}$
\end_inset


\end_layout

\begin_layout Standard
The intermediate probabilities are typically set to 
\begin_inset Formula $p_{k}(x)\propto p_{A}^{*}(x)^{1-\beta_{k}}.p_{B}^{*}(x)^{\beta_{k}}$
\end_inset

 where 
\begin_inset Formula $0=\beta_{0}<\beta_{1}<...<\beta_{K}=1$
\end_inset

.
 
\begin_inset Formula $\beta$
\end_inset

 is often referred to as the temperature coefficient since the distribution
 
\begin_inset Formula $p_{A}$
\end_inset

 is gradually 
\emph on
melted
\emph default
 into the distribution 
\begin_inset Formula $p_{B}$
\end_inset

.
 To compute the importance weights 
\begin_inset Formula $\omega$
\end_inset

 we run the following procedure
\end_layout

\begin_layout Enumerate
Sample from the initial distribution : 
\begin_inset Formula $x_{1}\sim p_{A}$
\end_inset


\end_layout

\begin_layout Enumerate
Generate the successive particles thanks to the transitions operator : 
\begin_inset Formula $x_{k+1}$
\end_inset

 given 
\begin_inset Formula $x_{k}$
\end_inset

 using 
\begin_inset Formula $T_{k}$
\end_inset

 for 
\begin_inset Formula $k$
\end_inset

 from 1 to 
\begin_inset Formula $K$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\omega=\frac{p_{1}^{*}(x_{1})}{p_{0}^{*}(x_{1})}\frac{p_{2}^{*}(x_{2})}{p_{1}^{*}(x_{2})}...\frac{p_{K}^{*}(x_{K})}{p_{K-1}^{*}(x_{K})}(\approx\frac{Z_{1}}{Z_{0}}\frac{Z_{2}}{Z_{1}}...\frac{Z_{K}}{Z_{K-1}})$
\end_inset


\end_layout

\begin_layout Standard
After M iteration, an approximation of the ratio of the two partition function
 is given by
\begin_inset Formula 
\[
\frac{Z_{B}}{Z_{A}}\approx\frac{1}{M}\sum_{i=1}^{M}\omega^{(i)}=\hat{r}_{AIS}
\]

\end_inset

The variance over the AIS estimator can be easily controlled 
\end_layout

\begin_layout Paragraph
AIS in a DBN
\end_layout

\begin_layout Subsection
Deep Boltzmann Machine
\end_layout

\begin_layout Standard
The joint training of layers in a DBN is problematic as the implied inference
 problem is often intractable.
 The Deep Boltzmann Machine (DBM) has been proposed to allow a joint training
 of all layers of deep architectures in a purely unsupervised manner 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2009deep"

\end_inset

.
 Similarly to the RBM, a DBM is a specific type of Boltzmann machine wirh
 layered hidden units.
 However, the DBM is composed of multiple layers, in which conditional independe
nce is imposed between odd-numbered layers and even-numbered layers (still
 without intra-layer connections).
 This allow DBM to directly learn increasingly complex representations but
 also resorting directly to massive amounts of unlabeled data.
 However, unlike DBN, their inference procedure can incorporate top-down
 feedback (because of undirected connections between layers), which allow
 to perform a better propagation of uncertainty.
\end_layout

\begin_layout Standard
A 2-layer DBM (with one visible layer and two hidden layers 
\begin_inset Formula $\left\{ \mathbf{v},\mathbf{h}^{1},\mathbf{h}^{2}\right\} $
\end_inset

 and parameters 
\begin_inset Formula $\theta$
\end_inset

) can be defined by specifying its energy function
\begin_inset Formula 
\[
E\left(\mathbf{v},\mathbf{h}^{1},\mathbf{h}^{2};\theta\right)=-\mathbf{v}^{T}\mathbf{W}^{1}\mathbf{h}^{1}-\mathbf{h}^{1T}\mathbf{W}^{2}\mathbf{h}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
Hence, the probability that the model assigns to a visible vector is given
 by
\begin_inset Formula 
\[
p\left(\mathbf{v},\theta\right)=\frac{1}{Z}\sum_{\mathbf{h}^{1},\mathbf{h}^{2}}\mbox{exp}\left(-E\left(\mathbf{v},\mathbf{h}^{1},\mathbf{h}^{2};\theta\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Even though the the conditional distributions over the visible units and
 the last hidden units are defined in a similar way than in RBM, the distributio
n over the 
\begin_inset Quotes eld
\end_inset

middle
\begin_inset Quotes erd
\end_inset

 layer of hidden units is defined as
\begin_inset Formula 
\[
p\left(h_{j}^{1}\mid\mathbf{v},\mathbf{h}^{2}\right)=\sigma\left(\sum_{i}W_{ij}^{1}v_{i}+\sum_{m}W_{jm}^{2}h_{j}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Hence, the major problem with DBM (as compared to RBM) is that these interaction
s between hidden units make the posterior of hidden units intractable.
 To solve this problem, 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2009deep"

\end_inset

 propose to rely on a mean-field approximation (similar to the ideas of
 
\emph on
variational learning
\emph default
), where the posterior distribution 
\begin_inset Formula $P\left(\mathbf{h}^{1},\mathbf{h}^{2}\mid\mathbf{v}\right)$
\end_inset

 is approximated with a factored distribution 
\begin_inset Formula $Q\left(\mathbf{h}^{1},\mathbf{h}^{2}\right)=\prod_{j}Q\left(h_{j}^{1}\right)\prod_{i}Q\left(h_{i}^{2}\right)$
\end_inset

 such that the KL divergence between the real and approximated distributions
 
\begin_inset Formula $KL\left(P\left(\mathbf{h}^{1},\mathbf{h}^{2}\mid\mathbf{v}\right)\parallel Q\left(\mathbf{h}^{1},\mathbf{h}^{2}\right)\right)$
\end_inset

 is minimized.
 This is equivalent to maximizing a lower bound on the log-likelihood
\begin_inset Formula 
\[
\mathcal{L}\left(Q\right)=\sum_{h^{1}}\sum_{h^{2}}Q\left(\mathbf{h}^{1},\mathbf{h}^{2}\right)\mbox{log}\left(\frac{P\left(\mathbf{h}^{1},\mathbf{h}^{2}\mid\mathbf{v}\right)}{Q\left(\mathbf{h}^{1},\mathbf{h}^{2}\right)}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
In order to maximize this lower-bound, developing the zero derivatives with
 respect to the approximated distribution 
\begin_inset Formula $Q\left(\mathbf{h}^{1},\mathbf{h}^{2}\right)$
\end_inset

, we obtain the update equations
\begin_inset Formula 
\[
h_{j}^{1}\leftarrow\sigma\left(\sum_{i}W_{ij}^{1}v_{i}+\sum_{m}W_{jm}^{2}h_{m}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The training phase is proposed through a variational procedure where the
 positive phase is modified but the negative phase can be estimated through
 contrastive divergence, similarly to RBM.
\end_layout

\begin_layout Subsubsection
Pre-training DBM's
\end_layout

\begin_layout Standard
A greedy layer-wise unsupervised pre-training step can be used to speed
 up the training phase by initializing the weights to sensible values and
 gives a fast approximate inference of the hidden units which ca be used
 to initialize the mean-field methods 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2010efficient,salakhutdinov2009learning"

\end_inset

.
\end_layout

\begin_layout Standard
Simply stacking RBM's on top of each other does not form a DBM.
 Instead, it forms an a model called Deep Belief Network (DBN) formed by
 a top undirected RBM (kind of a memory) and a directed sigmoid belief networks.
 Indeed, except the top two layers which form a undirected RBM, all the
 other group of layers are now composed by top-down connections since the
 last modifications their weights suffered was when they were considered
 as the visible units of an RBM.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2009deep"

\end_inset

, an example is given for a two hidden layer DBM.
 In a simple RBM the distribution of the visible units is given by 
\begin_inset Formula 
\[
p(v;\theta)=\sum_{h^{1}}p(h^{1};W^{1})p(v|h^{1};W^{1})
\]

\end_inset

where 
\begin_inset Formula $p(h^{1};W^{1})$
\end_inset

 is a prior over 
\begin_inset Formula $h^{1}$
\end_inset

 activations.
 If correctly trained, the second layer in the stack replace this prior
 by a better prior 
\begin_inset Formula $p(h^{1};W^{2})=\sum_{h^{2}}p(h^{1},h^{2};W^{2})$
\end_inset

.
 This is what is done in a deep belief network.
 In a DBM, 
\begin_inset Formula $h^{1}$
\end_inset

 depends on both 
\begin_inset Formula $W^{1}$
\end_inset

 and 
\begin_inset Formula $W^{2}$
\end_inset

, and it is possible to roughly approximate the prior over 
\begin_inset Formula $h^{1}$
\end_inset

 by averaging the two models
\begin_inset Formula 
\[
p(h^{1};W^{1},W^{2})=\frac{1}{2}\sum_{v}p(h^{1},v;W^{1})+\frac{1}{2}\sum_{h^{2}}p(h^{1},h^{2};W^{2})
\]

\end_inset


\end_layout

\begin_layout Standard
To eliminate the doubling effect, a solution consists in doubling the visible
 units.
 Hence, two matrices 
\begin_inset Formula $W^{1}$
\end_inset

 between visible and first layer hidden units are created.
 The weights of those to matrices are tied during the training phase.
 The same trick is used for the top-most RBM, where hidden last layer is
 duplicated.
 For the middle RBM, the weights are simply multiplied by two.
 Once separately trained, the RBM's are simply stacked by dividing the weights
 in the middle layer by two and taking only one of the two matrices for
 the extreme layers.
 The bottom and top most RBM's can not have their weights simply doubled,
 because when stacking the RBM's those layer won't have a lower (respectively
 upper) layer and their weights won't be doubled.
 Indeed, simply doubling the visible-to-first_hidden connections would have
 lead to 
\begin_inset Formula $p(v|h^{1})=\sigma(\bm{2}\sum_{j}W_{ij}^{1}h_{j}^{1})$
\end_inset

.
\end_layout

\begin_layout Standard
Those initial weights can be used to approximately infer the hidden units
 given a training vector, which is useful to speed-up the mean-field algorithm
 by initializing it in configuration closest to the fixed point than when
 a random initialization is used.
 Inference is performed through a simple bottom-up pass, and the weights
 of the matrices has to be doubled, to compensate the absence of top-down
 influences.
\end_layout

\begin_layout Standard
Intuitively, this can be explained by the fact that when trained separately,
 the RBM's compensate the lack of top-down input (respectively bottom-up)
 by doubling its weights.
 Replicating units or doubling weights 
\emph on
simulate
\emph default
 the presence of a top and bottom layer.
\end_layout

\begin_layout Subsubsection
Evaluating DBM's
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Subsection
Temporal models
\end_layout

\begin_layout Standard
Unsupervised feature learning and deep learning algorithms were initially
 developed for static data and features.
 Hence, in order to apply these methods to datasets with a clear temporal
 nature, these approaches should be adapted to fit the subtlety and challenges
 raised by time series datasets 
\begin_inset CommandInset citation
LatexCommand cite
key "langkvist2014review"

\end_inset

.
 
\end_layout

\begin_layout Standard
Time-series data are a peculiar object of study with several characteristics
 that distinguish them from other types of data.
 Firstly, the most prominent problems arise from the high dimensionality
 of time series data.
 This lead to the problem known as the 
\emph on
curse of dimensionality 
\emph default
which prevents traditional learning algorithms to decipher the inner correlation
s of temporal data.
 Second, time-series data often stem from a sampling process which induces
 noise in the observation.
 To alleviate this problem, signal processing techniques such as low-pass
 filtering or spectral analysis can be applied to remove some of this high-frequ
ency noise.
 Third, the explicit dependency on the temporal dimension entails a notion
 of temporal granularity in which the time dependencies can coexist at various
 scales.
 Finally, 
\bar under
there is a difference between time-series data and other types of data when
 it comes to invariance.
 Most features used for time-series need to be invariant to translations
 in time
\bar default
.
 
\series bold
+ REPLACE THIS WITH MY AXIOMS OF ROBUSTNESS
\end_layout

\begin_layout Standard
Different time-series datasets will usually exhibit different degrees of
 these peculiarities, which warrants the need of incorporating prior knowledge
 on these characteristics in the chosen learning framework.
 Hence, the feature extraction mechanisms have to be modified in order to
 capture temporal dependencies by adjusting to the properties of temporal
 data.
\end_layout

\begin_layout Standard
Real-world time-series data are often high-dimensional, noisy and pertain
 to generating mechanisms that cannot be modeled through analytical equations
 since their dynamics are either unknown or too complex to be formalized.
 
\bar under
it is not certain that there are enough information available to understand
 the process.
 
\series bold
NB: What was the rule in the line that 
\begin_inset Quotes eld
\end_inset

we need at least as many examples as there are dimensions of variation
\begin_inset Quotes erd
\end_inset

.

\series default
 Many time-series are also non-stationary, meaning that the characteristics
 of the data, such as mean, variance, and frequency
\bar default
 vary across different windows of the series
\bar under
.
 
\series bold
PARAPHRASE + USE OPPOSITION WITH THE STATIONARITY HYPOTHESIS
\series default
\bar default
 Hence, shallow methods typically applying a single layer of non-linear
 operations seem ill-suited to accurately model data of such complex nature.
\end_layout

\begin_layout Subsubsection
Recurrent Neural Network
\begin_inset CommandInset label
LatexCommand label
name "sub:Recurrent-Neural-Network"

\end_inset


\end_layout

\begin_layout Standard
Recurrent Neural Network (RNN) 
\begin_inset CommandInset citation
LatexCommand cite
key "pineda1987generalization"

\end_inset

 have been used for modeling temporal data long before the advent of deep
 learning.
 An RNN can simply be obtained by modifying a feedforward network in order
 to allow 
\begin_inset Quotes eld
\end_inset

loops
\begin_inset Quotes erd
\end_inset

 where the output of neurons are connected to their own inputs.
 These looped connections allow to model the short-term time-dependencies
 without using any time delay-taps.
 In order to solve the propagation indeterminacy when trying to update the
 weights in training phase, an iterative algorithm such as the backpropagation-t
hrough-time (BPTT) 
\begin_inset CommandInset citation
LatexCommand cite
key "werbos1990backpropagation"

\end_inset

 can be used.
 Alternative strategies for training RNNs can also be employed 
\begin_inset CommandInset citation
LatexCommand cite
key "sutskever2007learning"

\end_inset

.
 The input is transformed through the hidden units that have connections
 between input of the current time frame and output from the previous time
 frame (modeled by the loops in the connections).
 
\bar under
A popular extension is the use of the purpose-built Long-short term memory
 cell 
\begin_inset CommandInset citation
LatexCommand cite
key "hochreiter1997long"

\end_inset

 that better finds long-term dependencies.
\end_layout

\begin_layout Paragraph
LSTM
\end_layout

\begin_layout Paragraph
Depth-gated LSTM
\end_layout

\begin_layout Standard
Stacking LSTM is relatively straight-forward.
 The output of a memory cell at a layer 
\emph on
l
\emph default
 is simply rooted to the input of the memory cell at the layer 
\emph on
l+1
\emph default
.
 A more refined solution to build deep LSTM networks proposes to connect
 memory cells of adjacent layers via a gating function called 
\emph on
depth gate
\emph default
.
 This model is called a 
\emph on
depth-gated LSTM 
\begin_inset CommandInset citation
LatexCommand cite
key "DBLP:journals/corr/YaoCVDD15"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Convolution and pooling
\end_layout

\begin_layout Standard
Convolution offers a particularly interesting framework for high-dimensional
 data, such as time-series data.
 The fact that in convolutional networks, units in the hidden are 
\emph on
locally 
\emph default
connected to contiguous visible segments (instead of being fully connected)
 both provides a natural way to handle temporal continuity and an increased
 computational efficiency.
 Both the AE and RBM models have been extended to incorporate convolutional
 operators in order to produce convolutional AEs (convAE) 
\begin_inset CommandInset citation
LatexCommand cite
key "masci2011stacked"

\end_inset

 and convolutional RBMs (convRBM) 
\begin_inset CommandInset citation
LatexCommand cite
key "lee2009convolutional"

\end_inset

.
 Classical neural networks have been specialized to exploit the input time
 structure by performing convolutions on overlapping windows in an approach
 called Time-Delay Neural Network (TDNN) 
\begin_inset CommandInset citation
LatexCommand cite
key "waibel1989modular"

\end_inset

.
\end_layout

\begin_layout Standard
Along with convolution, which creates high-dimensional replicates of the
 inputs through different feature extractors, the pooling operation can
 combine locally contiguous input values or features through the application
 of an average, max or histogram operator.
 Pooling not only provides invariance to small distortions in the local
 neigborhood but also drastically reduces the dimensionality of the feature
 space.
 However, the major drawback of pooling is that it is non-differentiable.
 This can be alleviated by relying on the 
\emph on
probabilistic max-pooling 
\emph default
operator introduced by Lee et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "lee2009convolutional"

\end_inset

.
 Another proposed approach is the Space–Time DBN (ST-DBN) 
\begin_inset CommandInset citation
LatexCommand cite
key "chen2010deep"

\end_inset

based on convolutional RBMs in which a separate pooling layer is applied
 to the spatial and temporal dimensions separately to build spatio-temporally
 invariant features.
\end_layout

\begin_layout Subsubsection
Temporal coherence
\end_layout

\begin_layout Standard
Temporal coherence is one of the most important property of time series
 data and can be related to invariant feature representations.
 The goal in this case would be that small changes in the input data (contiguous
 time frames) only incur small changes in the feature representation, which
 can also be achieved using a structured form of sparsity penalty targeted
 at temporal relationships 
\begin_inset CommandInset citation
LatexCommand cite
key "kavukcuoglu2009learning"

\end_inset

.
\end_layout

\begin_layout Standard
Hence, asides from the previously introduced various architectural constructs
 that can be used to capture temporal dependencies, various types of smoothness
 penalties on the hidden variables can be introduced to enforce temporal
 regularization.
 This is usually done by penalizing the squared differences in the hidden
 unit activations between contiguous time frames.
 The main idea behind this type of penalty is that sequential data is supposed
 to be smooth, so the hidden activations should not vary much if this temporal
 data is fed to the model in a chronological order.
\end_layout

\begin_layout Subsubsection
Comparison
\end_layout

\begin_layout Standard
The major difference between models targeted at temporal data analysis lies
 in their implementation of the notion of temporal memory.
 In a cRBM, the short-term dependencies across visible units are modeled
 through delay taps and the longer-term dependencies are a by-product of
 subsequent layers modeling.
 Hence, the length of a cRBM memory can be increased by increasing the number
 of layers accounted in the model.
 In an RNN, the loop connections causes hidden units to be influenced by
 their state in the previous time frame.
 These connections can can create a 
\emph on
ripple
\emph default
 effect, which can last over a potentially infinite number of time frames.
 This ripple effect can be prevented through the addition of a forget gate
 
\begin_inset CommandInset citation
LatexCommand cite
key "gers2000learning"

\end_inset

 which periodically removes the effect of the feedback connection.
 By relying on Hessian-free optimizer 
\begin_inset CommandInset citation
LatexCommand cite
key "martens2012training"

\end_inset

 or the Long-short term memory 
\begin_inset CommandInset citation
LatexCommand cite
key "hochreiter1997long"

\end_inset

 models, recurrent networks can provide a longer-term memory spanning across
 more than a hundred time steps.
 
\bar under
The Gated RBM and the convolutional GRBM models transitions between pairs
 of input vectors so the memory for these models is 2
\bar default
.
\end_layout

\begin_layout Subsubsection
Summary
\end_layout

\begin_layout Standard
Based on the presented models, a number of problem-specific recommandations
 should be raised to investigate temporal data analysis.
 Hence, selecting the best-suited model for a specific problem first requires
 to take several questions into considerations
\end_layout

\begin_layout Enumerate

\emph on
The characteristics and underlying structure of the data
\emph default
.
 The type of pre-processing, choice of the models and even their parametrization
 should heavily depend on the inner characteristics of the studied data.
 If the data has an inherent dimension, the typical feature vector approach
 would discard any temporal relationship.
 Instead, a model targeted at exploiting temporal regularities through either
 notions of memory (by architectural structure) or temporal coherence (by
 regularization penalties) has a higher chance of providing satisfactory
 results.
\end_layout

\begin_layout Enumerate

\emph on
Relying on generative or discriminative models
\emph default
.
 A generative model provides a straightforward way of synthesizing new data
 but also predicting partial input data that would need to be reconstructed.
 Generative models are, therefore, usually more robust to noisy inputs and
 provide a better detection of outliers.
 On the other hand, discriminative models are more efficient for classification
 problems and also easier to implement.
\end_layout

\begin_layout Enumerate

\emph on
The dimensionality of the input data
\emph default
.
 For large-scale problems, stochastic gradient descent can provide a faster
 optimization method 
\begin_inset CommandInset citation
LatexCommand cite
key "bottou2010large"

\end_inset

.
 Massive datasets can also precludes the use of longer-term memory models
 and appropriate pre-processing methods should be applied.
\end_layout

\begin_layout Standard
The premise of deep networks is that a structured hierarchy of abstractions
 can be learned from the underlying distribution of data.
 This hypothesis fits the construct of musical data in which notes group
 into chords, temporally arranged to create melodies and rhythms which in
 turn makes motifs and phrases emerging to construct entire musical pieces
 
\begin_inset CommandInset citation
LatexCommand cite
key "humphrey2013feature"

\end_inset

.
 Hence, deep learning algorithms could target these elementary building
 blocks (musical motifs) and complex musical pieces could be constructed
 from a hierarchical structure of these previously learned motifs.
\end_layout

\begin_layout Standard

\bar under
Even though convolutional networks have given good results on time-frequency
 representations of audio, there is room for discovering new and better
 models.
\end_layout

\begin_layout Standard
Deep networks can, in an unsupervised manner, learn these motion templates
 from raw data and use them to form complex human motions.
\end_layout

\begin_layout Standard
Constructing features learning from raw data has been extensively studied
 in vision tasks but is yet to be attempted in music recognition.
 
\bar under
Models such as TDNN, cRBM and convolutional RBMs are well suited for being
 applied to raw data, but most works that construct useful features from
 the input data actually still use input data from pre-processed features
\end_layout

\begin_layout Subsection
Memory models
\end_layout

\begin_layout Standard
Memorizing sequences involving different temporal granularities seems to
 be troublesome for neural networks.
 Recent works have tried to address those issues, typically by increasing
 the complexity of the temporal architectures introduced before (
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Recurrent-Neural-Network"

\end_inset

).
 An other approach consists in modeling separately computing and memory
 units, as in a computer.
\end_layout

\begin_layout Subsubsection
Stacked-augmented recurrent nets
\end_layout

\begin_layout Standard
A stack structure is added to recurrent networks (simple RNN or LSTM).
 Hence, this memory is accessible only trough its top most element and manipulat
ed with the 
\emph on
push
\emph default
, and 
\emph on
pop 
\emph default
operations, plus a 
\emph on
no-op 
\emph default
instruction.
 This action is represented by a 3-dimensional variable 
\begin_inset Formula $a_{t}$
\end_inset

 which depends on the state of the hidden variable 
\begin_inset Formula $h_{t}$
\end_inset

 :
\begin_inset Formula 
\[
a_{t}=f(Ah_{t})
\]

\end_inset

Given the value of the stack, the hidden layer is updated as :
\begin_inset Formula 
\[
h_{t}=\sigma(Ux_{t}+Rh_{t-1}+\text{P}s_{t-1}^{k})
\]

\end_inset

 where 
\begin_inset Formula $P$
\end_inset

 is a 
\begin_inset Formula $m\times k$
\end_inset

 matrix and 
\begin_inset Formula $s_{t-1}^{k}$
\end_inset

 are the 
\begin_inset Formula $k$
\end_inset

 top-most element of the stack at time 
\begin_inset Formula $t-1$
\end_inset

.
 With 
\begin_inset Formula $a_{t}[PUSH]$
\end_inset

 being the probability of the PUSH action, the stack is modified as follows
\begin_inset Formula 
\[
s_{t}[0]=a_{t}[PUSH]\sigma(Dh_{t})+a_{t}[POP]s_{t-1}[1]+a_{t}[NO-OP]s_{t-1}[\text{0]}
\]

\end_inset

for the top of the stack, and for every other index
\begin_inset Formula 
\[
s_{t}[i]=a_{t}[PUSH]s_{t-1}[i-1]+a_{t}[POP]s_{t-1}[i+1]+a_{t}[NO-OP]s_{t-1}[\text{i]}
\]

\end_inset


\end_layout

\begin_layout Standard
FIGURES MOTHERFUCKER
\end_layout

\begin_layout Standard
This model can be extended in several ways
\end_layout

\begin_layout Itemize
Multiple stacks in parallel
\end_layout

\begin_layout Itemize
Other data structure for the memory : lists, heap
\end_layout

\begin_layout Itemize
Optimization
\end_layout

\begin_layout Standard
Note that as pointed out in the article, 
\emph on
LSTM 
\emph default
actually implement a kind of memory through its forget gate and is then
 able to count occurrences of different events.
\end_layout

\begin_layout Subsection
Multimodal models
\end_layout

\begin_layout Section
Other models
\end_layout

\begin_layout Subsection
Convolutional Neural Networks
\end_layout

\begin_layout Standard
Convolutional Neural Networks (CNN) are variants of Multi-Layer Perceptrons
 (MLPs) inspired by the works on the architecture of the visual cortex 
\begin_inset CommandInset citation
LatexCommand cite
key "hubel1968receptive"

\end_inset

, where the cells seem to be sensitive only to small sub-regions of the
 visual field.
 These regions, called 
\emph on
receptive fields
\emph default
 are targeted to exploit the local spatial regularities by acting as local
 filters over the input.
 Hence, each simple cell will respond maximally to a local region of space
 within this receptive field.
 Oppositely, more complex cells have a response to larger spatial regions
 and appear to provide an amount of spatial invariance to position and transform
ation of the patterns.
 These ideas were developed in convolutional form of neural networks such
 as LeNet 
\begin_inset CommandInset citation
LatexCommand cite
key "lecun1998gradient"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Sparse connectivity
\end_layout

\begin_layout Standard
Traditionnal neural networks are built on fully connected layers, where
 all hidden units are linked to all input units, which can imply a very
 high computational cost when working on large datasets with high dimensionaliti
es.
 However, as proposed by the work on the visual cortex, some neurons seem
 to target only local regions, to exploit the strong spatially-local correlation
 present in an image.
 These ideas could be easily reflected by restricting the connections of
 hidden units and enforcing local connectivity patterns between the input
 and hidden units.
 Hence, each hidden unit in one layer would only be connected to a small
 contiguous subset of units from the layer below.
 This mechanism mimics the idea that these neurons only focus on a local
 region of spatially contiguous information.
\end_layout

\begin_layout Standard
This type of sparse connectivity ensure that the units only respond to variation
s inside their own receptive field.
 Therefore, the 
\begin_inset Quotes eld
\end_inset

filters
\begin_inset Quotes erd
\end_inset

 learned are only sensitive to spatial locality.
 Furthermore, this strongly reduces the number of weights and parameters
 to be learned, which enhance the learning efficiency of the network.
\end_layout

\begin_layout Standard

\series bold
+ FIGURE
\end_layout

\begin_layout Subsubsection
Convolutions
\end_layout

\begin_layout Standard
CNNs exploit the property of 
\emph on
stationnarity
\emph default
, in which the underlying statistics of one part of the input are equivalent
 to the statistics of any other part.
 This property imply that a feature learned at a specific location will
 be useful for any other location.
 Therefore, the same local features learned from random subsets of the complete
 input can be applied at all the locations in the input.
 This operation is similar to the 
\emph on
convolution 
\emph default
operator, by applying this local feature (seen as a 
\emph on
kernel
\emph default
) over each location of the input.
\end_layout

\begin_layout Standard
More formally, if we learn a set of 
\begin_inset Formula $k$
\end_inset

 features from local connectivity sub-parts of size 
\begin_inset Formula $s_{x}\times s_{y}$
\end_inset

, we can convolve this feature over the complete input of size 
\begin_inset Formula $l_{x}\times l_{y}$
\end_inset

 to obtain an array of convolved features of size 
\begin_inset Formula $k\times(l_{x}-s_{x}+1)\times(l_{y}-s_{y}+1)$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Shared weights
\end_layout

\begin_layout Standard
Based on this construct, we can see that each local filter will be applied
 across the entire input.
 This lead to a form of replication where a set of units perform the same
 operation at different locations.
 Therefore, these units have the same set of parameters (weights and bias)
 which lead to a form of 
\emph on
weight sharing
\emph default
.
 In that case, the gradient can be computed efficiently only once for a
 unit and then summed across the number of replicated units.
\end_layout

\begin_layout Standard
This replication leads to increasing the robustness of the network to variance
 in position of the input, but also greatly improves the learning efficiency
 of the network by reducing its number of parameters.
\end_layout

\begin_layout Subsubsection
Feature maps
\end_layout

\begin_layout Standard
The replication of local filters applied across all regions of the input
 leads to a convolved array of features called a 
\emph on
feature map
\emph default
.
 More formally, given one local filtering unit 
\begin_inset Formula $k$
\end_inset

 parametrized by its weights 
\begin_inset Formula $W_{k}$
\end_inset

 and bias 
\begin_inset Formula $b_{k}$
\end_inset

, the feature map 
\begin_inset Formula $h^{k}$
\end_inset

 is obtained by applying the non-linear activation function to the local
 application of the feature to a specific location 
\begin_inset Formula $\left(i,j\right)$
\end_inset

 in the input 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
h_{ij}^{k}=\phi\left(\left(W_{k}*x\right)_{ij}+b_{k}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
In order to obtain multiple features from the image (and, therefore a more
 detailed representation of the data), the hidden layers are composed of
 multiple feature maps 
\begin_inset Formula $\left\{ h^{k},k=0\ldots N\right\} $
\end_inset

.
 Each of these feature maps provide the value of the learned features at
 all points in the input.
\end_layout

\begin_layout Standard

\series bold
+ FIGURE + Pooling figure
\end_layout

\begin_layout Subsubsection
Max pooling
\end_layout

\begin_layout Standard
These features maps extracted using convolution could theoretically be used
 altogether directly as input to a classifying layer such as a softmax classifie
r.
 However, given the obtained replication and largely increased dimensionality
 this can turn out to be computationally challenging.
 However, the original hypothesis of 
\emph on
stationarity
\emph default
 still holds and implies that features are likely to be similarly useful
 in any region of the input.
\end_layout

\begin_layout Standard
Based on these properties, it would seem logical to aggregate the values
 of the features across different positions in the global input.
 This would summarize the presence of the different features in increasingly
 large regions of the input.
 This form of down-sampling operation is called 
\emph on
pooling
\emph default
, which can be performed through the average (mean-pooling) or maximum (max-pool
ing) of the values inside a region.
 This operation partitions the input into a set of (non-overlapping) regions
 and outputs the corresponding value at each corresponding sub-region.
\end_layout

\begin_layout Standard
The pooling operation provides many attractive properties.
 First, as it eliminates the non-maximal values, it reduces the dimensionality
 and, therefore, also reduces the required computations in upper layers.
 Second, as the activation of a maximal value will remain identical with
 slight shifts (in the dimension of the contiguous region), it provides
 a degree of translation invariance.
 Hence pooling can be seen as an efficient non-linear dimensionality reduction
 and invariance technique.
\end_layout

\begin_layout Subsubsection
Tying the full model together
\end_layout

\begin_layout Standard
By stacking several layers of these local non-linear filters we can thus
 construct a deep network.
 As each layer is locally connected to the layer below, it can be seen as
 a form of local grouping.
 Therefore, the complete network provides increasingly global processing.
\end_layout

\begin_layout Standard
In order to tie the model together, lower-layers will be composed by alterning
 sparsely connected layers (performing convolution and producing the features
 map) and pooling layers (that regroup these features into regions).
 Based on this increasingly global analysis of various features, the final
 layers can be devised as a traditionnal MLP, with densely connected units
 aimed at performing discrimination.
\end_layout

\begin_layout Standard

\series bold
+ FIGURE
\end_layout

\begin_layout Subsubsection
Choosing hyperparameters
\end_layout

\begin_layout Standard
Even though CNNs provide a more expressive and versatile learning framework,
 they also imply a far larger number of hyper-parameters than a standard
 MLP.
 Hence, they require an amount of caution and experience when selecting
 their parameters.
\end_layout

\begin_layout Paragraph
Filters
\end_layout

\begin_layout Standard
The CNNs are heavily conditionned on the number of filters used at each
 layer.
 A quite logical thinking would be to assume that the more features are
 computed, the more equipped the network will be to deal with any type of
 situation.
 However, a single convolutional layer is not only strongly more expensive
 to compute (in terms of forward activations) than usual units, but also
 demultiplies the dimensionality of the input by the number of filters used.
 Hence, for a layer of 
\begin_inset Formula $K$
\end_inset

 features with an input of size 
\begin_inset Formula $l_{x}\times l_{y}$
\end_inset

 and filters of size 
\begin_inset Formula $s_{x}\times s_{y}$
\end_inset

, this layer will require 
\begin_inset Formula $k\times s_{x}\times s_{y}\times(l_{x}-s_{x})\times(l_{y}-s_{y})$
\end_inset

 operations.
 Therefore, the number of filters is usually picked so that this number
 of operations is almost equivalent to that of a traditionnal sigmoid layer.
 It should be noted however that this implies a trade-off between the capacity
 of the network and its computational complexity.
\end_layout

\begin_layout Standard
The shape of filters (in terms of dimensions) can also widely impact performance
s.
 This usually depends on the nature of the dataset at hand.
 The ideal filter shape is conditionned by a form of 
\emph on
granularity 
\emph default
depending on the amount and extent of spatially local correlations.
 Hence, the type of abstractions that are sought by the network should directly
 influence the scale of the filters.
\end_layout

\begin_layout Paragraph
Pooling
\end_layout

\begin_layout Standard
The pooling operation defines the amount of invariance provided by the network.
 However, it should be reminded that this is also a form of lossy compression
 that will strongly reduce the dimensionality of the input.
 This will provide computational efficiency but at the cost of loosing part
 of the exact locational information.
 Regarding the pooling function, it is widely accepted that the 
\emph on
max 
\emph default
operation appears to be the most robust as compared to the average.
\end_layout

\begin_layout Subsection
Sparse coding
\end_layout

\begin_layout Standard
The aim of sparse coding 
\begin_inset CommandInset citation
LatexCommand cite
key "olshausen1997sparse"

\end_inset

 is to find a decomposition of an input vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 as being a linear combination of a set of basis vectors 
\begin_inset Formula $\mathbf{\phi}_{i}$
\end_inset

, weighted by a set of activation weights 
\begin_inset Formula $a_{i}$
\end_inset

 
\begin_inset Formula 
\[
\mathbf{x}=\sum_{i=1}^{k}a_{i}\mathbf{\phi}_{i}
\]

\end_inset


\end_layout

\begin_layout Standard
Simlarly to autoencoders, we wish to learn a set of basis vectors which
 is over-complete.
 This can be understood as the goal to better capture a wider set of higher-leve
l structures and patterns underlying to the distribution of input data.
 However, this over-completeness also introduces the risk of a degeneracy
 in which nothing prevents the network from learning the identity function.
 Therefore, the additional constraint of sparsity is introduced to alleviate
 this risk and the sparse coding cost function on a set of m input vectors
 becomes 
\begin_inset Formula 
\[
\text{minimize}_{a_{i}^{(j)},\mathbf{\phi}_{i}}\sum_{j=1}^{m}\left|\left|\mathbf{x}^{(j)}-\sum_{i=1}^{k}a_{i}^{(j)}\mathbf{\phi}_{i}\right|\right|^{2}+\lambda\sum_{i=1}^{k}S(a_{i}^{(j)})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $S:\mathbb{R}\rightarrow\mathbb{R}$
\end_inset

 is a sparsity penalty which prevents the 
\begin_inset Formula $a_{i}$
\end_inset

 to be far from zero.
 Even though the most natural sparsity measure would be to use the 
\begin_inset Formula $L^{0}$
\end_inset

 norm (
\begin_inset Formula $S(a_{i})=\mathbf{1}(|a_{i}|>0)$
\end_inset

), it is non-differentiable and the 
\begin_inset Formula $L^{1}$
\end_inset

 (
\begin_inset Formula $S(a_{i})=\left|a_{i}\right|_{1}$
\end_inset

) or log (
\begin_inset Formula $S(a_{i})=\log(1+a_{i}^{2})$
\end_inset

) penalties are preferred.
 Contrarily to the AE or RBM cost where the reconstruction is bounded by
 the direct use of the input, sparse coding allow to optimize simultaneously
 the basis vectors 
\begin_inset Formula $\phi_{i}$
\end_inset

 and the activation 
\begin_inset Formula $a_{i}$
\end_inset

.
 This also imply that the sparsity penalty could be artificially small by
 scaling the 
\begin_inset Formula $\phi_{i}$
\end_inset

 by a large constant, which would arbitrarily scale down the 
\begin_inset Formula $a_{i}$
\end_inset

 (and therefore the sparsity).
 To prevent this behavior, we need to constraint the magnitude of the basis
 vectors 
\begin_inset Formula $\left|\left|\mathbf{\phi}\right|\right|^{2}$
\end_inset

 to remain under a constant 
\begin_inset Formula $C$
\end_inset

, which leads to the complete cost function
\bar under
 
\bar default

\begin_inset Formula 
\[
\begin{array}{rc}
\text{minimize}_{a_{i}^{(j)},\mathbf{\phi}_{i}} & \sum_{j=1}^{m}\left|\left|\mathbf{x}^{(j)}-\sum_{i=1}^{k}a_{i}^{(j)}\mathbf{\phi}_{i}\right|\right|^{2}+\lambda\sum_{i=1}^{k}S(a_{i}^{(j)})\\
\text{subject to} & \left|\left|\mathbf{\phi}_{i}\right|\right|^{2}\leq C,\forall i=1,...,k
\end{array}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Probabilistic interpretation
\end_layout

\begin_layout Standard
By interpreting the sparse coding in a probabilistic framework, we see that
 its goal is that the distribution 
\begin_inset Formula $P\left(\mathbf{x}\mid\mathbf{\phi}\right)$
\end_inset

 of the reconstruction of the input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 given a set of basis vectors 
\begin_inset Formula $\phi$
\end_inset

 is closest to the distribution of the data 
\begin_inset Formula $P\left(\mathbf{x}\right)$
\end_inset

.
 This goal can be achieved by minimizing the KL divergence between the two
 distributions 
\begin_inset Formula $KL\left(P\left(\mathbf{x}\mid\mathbf{\phi}\right)\parallel P\left(\mathbf{x}\right)\right)$
\end_inset

.
 To obtain the distribution of the reconstruction 
\begin_inset Formula $P\left(\mathbf{x}\mid\mathbf{\phi}\right)$
\end_inset

 we first need to define the prior distribution 
\begin_inset Formula $P\left(\mathbf{a}\right)$
\end_inset

 that can be factorized by assuming independence of source features
\begin_inset Formula 
\[
P(\mathbf{a})=\prod_{i=1}^{k}P(a_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
Then the probability of the data can be written under the reconstruction
 performed by 
\begin_inset Formula $\phi$
\end_inset

 and 
\begin_inset Formula $\mathbf{a}$
\end_inset

 such that
\bar under
 
\bar default

\begin_inset Formula 
\[
P(\mathbf{x}\mid\mathbf{\phi})=\int P(\mathbf{x}\mid\mathbf{a},\mathbf{\phi})P(\mathbf{a})d\mathbf{a}
\]

\end_inset


\end_layout

\begin_layout Standard
Finally, the sparse coding problem is to find the set of basis vectors that
 maximizes this probability, which is usually performed using the log-likelihood
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathbf{\phi}^{*}=\text{argmax}_{\mathbf{\phi}}\left(\mathbb{E}_{\mathbf{x}}\left[\log(P(\mathbf{x}\mid\mathbf{\phi}))\right]\right)
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\begin_inset Formula $\mathbb{E}_{\mathbf{x}}$
\end_inset

 defining the expectation over the data.
 Given this log-likelihood definition (which is equivalent to minimizing
 the energy), we can obtain the original formulation of the problem
\bar under
 
\bar default

\begin_inset Formula 
\[
\mathbf{\phi}^{*},\mathbf{a}^{*}=\text{argmin}_{\mathbf{\phi},\mathbf{a}}\sum_{j=1}^{m}\left|\left|\mathbf{x}^{(j)}-\sum_{i=1}^{k}a_{i}^{(j)}\mathbf{\phi}_{i}\right|\right|^{2}+\lambda\sum_{i=1}^{k}S(a_{i}^{(j)})
\]

\end_inset


\end_layout

\begin_layout Standard
An interesting variation of sparse coding is the 
\emph on
Spike-and-Slab Sparse Coding 
\emph default
(S3C) 
\begin_inset CommandInset citation
LatexCommand cite
key "goodfellow2012large"

\end_inset

 proposed for feature learning.
 This model adds a set of latent 
\emph on
spike 
\emph default
binary variables and a set of 
\emph on
slab 
\emph default
real-valued variables, where the activations of 
\emph on
spikes 
\emph default
directly controls the sparsity of the model.
\end_layout

\begin_layout Subsubsection
Autoencoder interpretation
\end_layout

\begin_layout Standard
Sparse coding can be interpreted as a sparse autoencoder in which both the
 set of sparse features useful for representing the data, and the basis
 for projecting from the feature space to the data space are learned simultaneou
sly.
 Within this framework, the objective function can be written as
\begin_inset Formula 
\[
\mathcal{J}(A,s)=\left\Vert As-x\right\Vert {}_{2}^{2}+\lambda\left\Vert s\right\Vert {}_{1}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\left\Vert \mathbf{x}\right\Vert {}_{k}=\left(\sum\left|x_{i}^{k}\right|\right)^{\frac{1}{k}}$
\end_inset

 refers to the 
\begin_inset Formula $L^{k}$
\end_inset

 norm of the vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Because of the previously mentionned problem of potential scaling on the
 sparsity constraint, the additional constraint to ensure the scale of 
\begin_inset Formula $A$
\end_inset

 expressed as 
\begin_inset Formula $A_{j}^{T}A_{j}\le1$
\end_inset

 is required.
 The complete sparse coding problem is expressed ass
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{array}{rcl}
{\rm minimize} & \left\Vert As-x\right\Vert {}_{2}^{2}+\lambda\left\Vert s\right\Vert {}_{1}\\
{\rm s.t.} & A_{j}^{T}A_{j}\le1\;\forall j
\end{array}
\]

\end_inset


\end_layout

\begin_layout Standard
As we can see, if we consider that 
\begin_inset Formula $A$
\end_inset

 is fixed, the problem of finding 
\begin_inset Formula $s$
\end_inset

 that minimizes the cost 
\begin_inset Formula $\mathcal{J}(s)$
\end_inset

 is convex.
 Reciprocally, if 
\begin_inset Formula $s$
\end_inset

 is fixed, the problem of minimizing 
\begin_inset Formula $\mathcal{J}(A)$
\end_inset

 is also convex.
 This points to a potential resolution algorithm that could alternately
 optimize one of these two parameters while considering the other fixed
 (optimize 
\begin_inset Formula $A$
\end_inset

 for a fixed 
\begin_inset Formula $s$
\end_inset

, then optimize 
\begin_inset Formula $s$
\end_inset

 for a fixed 
\begin_inset Formula $A$
\end_inset

).
 To obtain this simple optimization method, we must however handle the additionn
al scaling constraint on 
\begin_inset Formula $A$
\end_inset

.
 Hence, this constraint is often weakened to a term similar to the idea
 of 
\emph on
weight decay 
\emph default
penalty that constrains the values of A to remain small.
 This leads to the objective function
\begin_inset Formula 
\[
\mathcal{J}(A,s)=\left\Vert As-x\right\Vert {}_{2}^{2}+\lambda\left\Vert s\right\Vert {}_{1}+\gamma\left\Vert A\right\Vert {}_{2}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
As the 
\begin_inset Formula $L^{1}$
\end_inset

 norm used in the sparsity constraint contains a discontinuity at 0, it
 is not differentiable at 0 which is problematic for gradient-based methods.
 Hence, it is usually smoothed out by using the approximation 
\begin_inset Formula $\left|x\right|\approx\sqrt{x^{2}+\epsilon}$
\end_inset

 , where 
\begin_inset Formula $\epsilon$
\end_inset

 is considered as a very small value called 
\emph on
smoothing parameter
\emph default
.
 The final objective is then 
\begin_inset Formula 
\[
\mathcal{J}(A,s)=\left\Vert As-x\right\Vert {}_{2}^{2}+\lambda\left(\sum_{k}\sqrt{s_{k}^{2}+\epsilon}\right)+\gamma\left\Vert A\right\Vert {}_{2}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
This final formulation of the objective function can be optimized iteratively,
 by alternating between finding 
\begin_inset Formula $s$
\end_inset

 that minimizes 
\begin_inset Formula $\mathcal{J}\left(A,s\right)$
\end_inset

 for a fixed 
\begin_inset Formula $A$
\end_inset

 and then finding 
\begin_inset Formula $A$
\end_inset

 that minimizes 
\begin_inset Formula $\mathcal{J}\left(A,s\right)$
\end_inset

 for a fixed 
\begin_inset Formula $s$
\end_inset

.
 The procedure is
\end_layout

\begin_layout Enumerate
Perform a random initialization of 
\begin_inset Formula $A$
\end_inset


\end_layout

\begin_layout Enumerate
Repeat until convergence 
\end_layout

\begin_deeper
\begin_layout Enumerate
Find through gradient descent the 
\begin_inset Formula $s$
\end_inset

 that minimizes 
\begin_inset Formula $J(A,s)$
\end_inset

 given the current 
\begin_inset Formula $A$
\end_inset


\end_layout

\begin_layout Enumerate
Analytically solve the 
\begin_inset Formula $A$
\end_inset

 that minimizes 
\begin_inset Formula $J(A,s)$
\end_inset

 for the current 
\begin_inset Formula $s$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
However, simply applying this algorithm as it is will usually not produce
 satisfactory results, as two main tricks are required to achieve better
 convergence.
 First, as each iteration takes a long time before convergence, the algorithm
 is instead run at each iteration solely on a 
\emph on
mini-batch
\emph default
 (a different randomly chosen subset of the dataset).
\end_layout

\begin_layout Standard
Second, the random initialization of the basis vectors 
\begin_inset Formula $s$
\end_inset

 at each iteration might lead to slow convergence (as it might be initialized
 in a sub-optimal region of the space).
 This problem can be alleviated by performing a data-sensitive initialization
 of s
\end_layout

\begin_layout Enumerate
First, setting 
\begin_inset Formula $s=W^{T}\mathbf{x}$
\end_inset

 with 
\begin_inset Formula $\mathbf{x}$
\end_inset

 being the set of input vectors.
\end_layout

\begin_layout Enumerate
Then, for each input dimension, we divide it by the norm of the corresponding
 (i.e.
 for the same dimension) basis vector.
\end_layout

\begin_layout Standard
This type of initialization attempts to start by finding a better approximation
 of 
\begin_inset Formula $s$
\end_inset

 (by applying the reverse transform from 
\begin_inset Formula $Ws\approx x$
\end_inset

).
 Then, the second step normalize the initialization to ensure that the sparsity
 penalty is kept small.
\end_layout

\begin_layout Subsubsection
Topographic sparse coding
\end_layout

\begin_layout Standard
It might be desirable to learn an 
\emph on
ordered
\emph default
 set of features meaning that adjacent neurons detect similar features (which
 mimics the actual layout of biological neurons in the brain), leading to
 a notion of 
\emph on
topographic ordering
\emph default
.
 Intuitively, if feature detectors are laid out in neighborhood depending
 on their similarities, then activation of a feature will imply an activation
 of its neighbor (to a lesser extent).
 Hence, we would like adjacent units of the network to organize into similar
 features.
 This can be obtained by adding a smoothed 
\begin_inset Formula $L^{1}$
\end_inset

 penalty on the activation of the features over sub-regions of the network.
 Hence, by summing over groups of features we ensure that only localized
 groups activate for each example, leading to the overall cost function
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mathcal{J}(A,s)=\left\Vert As-x\right\Vert {}_{2}^{2}+\lambda\sum_{\text{all groups }g}\sqrt{\left(\sum_{\text{all }s\in g}s^{2}\right)+\epsilon}+\gamma\left\Vert A\right\Vert {}_{2}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
To easily compute this topographic penalty, we can rely on a 
\emph on
grouping matrix 
\emph default

\begin_inset Formula $V$
\end_inset

 which indicate for each row the membership of a feature to a particular
 group.
 This allows to simplify the computation of the gradients and provide an
 elegant way of writing the objective function as
\begin_inset Formula 
\[
J(A,s)=\left\Vert As-x\right\Vert {}_{2}^{2}+\lambda\sum_{r}\sum_{c}D_{r,c}+\gamma\left\Vert A\right\Vert {}_{2}^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $D=\sqrt{Vss^{T}+\epsilon}$
\end_inset

.
\end_layout

\begin_layout Subsection
Deconvolutional networks
\end_layout

\begin_layout Standard
Deconvolutional networks are generative models which can be seen as a convolutio
nal form of sparse coding.
 One of the major difference is that the pooling operation is differentiable
 and directly integrated into the cost function via latent variables.
\end_layout

\begin_layout Subsubsection
Single layer
\end_layout

\begin_layout Standard
The application of sparse coding yield an over-complete linear decomposition
 of an input 
\begin_inset Formula $\mathbf{x}$
\end_inset

 using a set of basis vectors 
\begin_inset Formula $\phi$
\end_inset

 (with a 
\begin_inset Formula $L^{1}$
\end_inset

 regularization to enforce sparsity).
 The convolutional network is almost the exact opposite as this supervised
 and encoding-only definition of sparse coding (by performing the steps
 of convolution, non-linearity and pooling).
 De-convolution is based on trying to follow the opposite path which seeks
 to find the feature maps (by trying to find the unpooling variables) to
 reconstruct the input.
 Hence, a de-convolutional network is a decoding trying to infer the features
 by
\end_layout

\begin_layout Enumerate
Unpooling the feature maps (using inferred latent variables)
\end_layout

\begin_layout Enumerate
Convolving the unpooled maps (learned filters) to the input
\end_layout

\begin_layout Enumerate
Performing an unsupervised reconstruction of the input with sparsity constraint
\end_layout

\begin_layout Paragraph
Gaussian unpooling
\end_layout

\begin_layout Standard
The principle behind de-convolutional networks is to try to 
\emph on
unpool 
\emph default
the observed features.
 Hence, each unpooling region can be seen as a two-dimensional Gaussian
 with weights scaled by feature map activation.
 The advantage of this formulation is that it leads to a differentiable
 representation (with the unpooling variables being the mean and precisions
 of the Gaussians, with one laid out for each feature map value).
\end_layout

\begin_layout Paragraph
Cost function
\end_layout

\begin_layout Standard
The cost function for de-convolutional networks is closely similar to that
 of sparse coding
\begin_inset Formula 
\[
\frac{\lambda}{2}\left\Vert YU_{\theta}f-x\right\Vert _{2}^{2}+\left|f\right|_{\frac{1}{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
with 
\begin_inset Formula $x$
\end_inset

 the input, 
\begin_inset Formula $f$
\end_inset

 the feature maps, 
\begin_inset Formula $Y$
\end_inset

 the reconstructed input and 
\begin_inset Formula $U_{\theta}$
\end_inset

 is the unpooling operation (parametrized by the means and precisions of
 the Gaussians).
\end_layout

\begin_layout Paragraph
Single layer inference
\end_layout

\begin_layout Standard
The learning procedure of de-convolutional networks is also closely related
 to the one used for sparse coding.
 Indeed, a two-step procedure is used where one aspect is learned while
 the others are considered fixed.
 The inference procedure tries to find
\end_layout

\begin_layout Itemize
Feature maps 
\begin_inset Formula $p$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

what
\begin_inset Quotes erd
\end_inset

 aspect) are infered by fixing the convolution filters and pooling variables
 
\begin_inset Formula $\theta$
\end_inset

 (this amounts to a convolutional form of sparse coding)
\end_layout

\begin_layout Itemize
Pooling variables 
\begin_inset Formula $\theta$
\end_inset

 (the 
\begin_inset Quotes eld
\end_inset

where
\begin_inset Quotes erd
\end_inset

 aspect) are infered by fixing the filters and feature maps, then running
 a chain rule of derivatives to update the mean and precision of each Gaussian.
\end_layout

\begin_layout Subsubsection
Multi-layer stacking
\end_layout

\begin_layout Standard
In order to produce a deep (multi-layer) deconvolutional network, the idea
 is to use the pooled maps as input to next layer, but performing a joint
 inference over all layers (which is only possible thanks to the differentiable
 pooling operation).
 With the objective still being to reconstruct the input image (to rely
 on the reconstruction error).
 So the idea is still to minimize the reconstruction error of the input
 image subject to sparsity.
 So multi-layer inference can be done by updating feature maps at top but
 also pooling variables from 
\emph on
both
\emph default
 layers.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Applying deep learning
\end_layout

\begin_layout Subsection
Preprocessing
\end_layout

\begin_layout Standard
The first step prior to trying to apply any learning mechanism to a specific
 problem is to understand the data itself and carefully try to understand
 its properties and inner structure.
 Indeed, datasets of different nature will call for different pre-processing
 methods.
 For instance, the 
\emph on
stationnarity 
\emph default
property which is heavily used in vision tasks does not necessarily hold
 for all types of data.
 Common pre-processing methods for data normalization include 
\end_layout

\begin_layout Itemize
Rescaling the input data vectors can allow to bound the data in a given
 range (usually 
\begin_inset Formula $[0,1]$
\end_inset

 or 
\begin_inset Formula $[−1,1]$
\end_inset

) to ensure the correct use of different activation functions
\end_layout

\begin_layout Itemize
Mean subtraction of examples is used when the data is 
\emph on
stationary
\emph default
 (the statistics of each dimension follow the same distribution).
 Subtracting the mean-value of each training example can drive the algorithm
 towards focusing on the variation of data rather than its absolute values
\end_layout

\begin_layout Itemize
Feature standardization (through zero-mean, unit-variance transformation)
 for each dimension across the dataset can allow to avoid that one feature
 with a wider variation range overshadows the other components.
 Hence, to balance the impact of each components, their values can be rescaled
 independently.
\end_layout

\begin_layout Standard
Furthermore, dimensionality reduction techniques also provides the advantage
 to significantly speed up the subsequent learning algorithm.
 As a structured input is usually somewhat redundant (an amount of correlation
 is always present in non-random data), can remove these correlations to
 provide a lower dimensional version of the input, while incurring only
 low amounts of error.
\end_layout

\begin_layout Subsubsection
Signal
\end_layout

\begin_layout Subsubsection
Symbolic
\end_layout

\begin_layout Paragraph
Pianoroll
\end_layout

\begin_layout Standard
The most standard representation for symbolic music is probably the pianoroll,
 which naturally flows from the discrete representation of time and pitches
 underlying in every written piece of music.
 In occidental music, this discretization is embodied by the beat and the
 placement of notes on staves.
 Hence, a pianoroll is matrix whose column represent successive time frames
 and lines are associated with a pitch.
 If the pitch discretization is often an half tone, many different time
 scale can be used to define the pianoroll.
 The behavior of a system built on this representation will be highly dependent
 on this time quantification and this is a hot point.
\end_layout

\begin_layout Paragraph
From a sparse binary representation to a real-valued distributed representation
 ?
\end_layout

\begin_layout Standard
Pianoroll representations leads for each time frame to highly sparse vectors,
 which is not a desirable property for the input data of a network.
 It would be more interesting to work with a distributed real-valued representat
ion which would be more compact and meaningful.
 In Natural Language Processing (NLP) the most basic representation consists
 in encoding each word in a vector of the size of the vocabulary with one
 unit active.
 The log-bilinear model (LBL, 
\begin_inset CommandInset citation
LatexCommand cite
key "mnih2007three"

\end_inset

) allows to find the most adapted representation to a context based prediction
 task while reducing its dimensionality.
 Besides, it has been shown that it leads to a representation where basic
 algebraic operations convey a semantic meaning.
 The word representation is defined by a representation matrix 
\emph on
R 
\emph default
of size 
\emph on

\begin_inset Formula $V\times D$
\end_inset


\emph default
 where 
\emph on
V
\emph default
 is the size of the vocabulary and 
\emph on
D
\emph default
 is the word representation size.
 This matrix 
\emph on
R 
\emph default
then represent the mapping from the one-hot representation 
\begin_inset Formula $w$
\end_inset

 to the distributed representation 
\begin_inset Formula $r_{\{w\}}$
\end_inset


\begin_inset Formula 
\[
r_{\{w\}}=R.w
\]

\end_inset

Given a sequence of words 
\begin_inset Formula $w_{\{1\}},...,w_{\{n-1\}}$
\end_inset

, the LBL model linearly predicts the next word representation
\begin_inset Formula 
\[
\hat{r}=\sum_{i=1}^{n-1}C^{(i)}r_{w_{i}}
\]

\end_inset

where 
\begin_inset Formula $C^{(i)},$
\end_inset

 
\begin_inset Formula $i=1,...,n-1$
\end_inset

 are 
\begin_inset Formula $D\times D$
\end_inset

 context parameter matrices.
 The predictive probability is then given by as softmax layer 
\begin_inset Formula 
\[
P(w_{n}|w_{1},...,w_{n-1})=\frac{{\exp(\hat{r}^{T}r_{i}+b_{i})}}{\sum_{j=1}^{V}\exp(\hat{r}^{T}r_{j}+b_{j})}
\]

\end_inset

where 
\begin_inset Formula $\mathbf{b}\in\mathbb{R}^{V}$
\end_inset

 is the bias vector.
 The gradients for maximum likelihood learning are then given by
\begin_inset Formula 
\[
\frac{\partial}{\partial C^{(i)}}\log\left[P(w_{n}|w_{1},...,w_{n-1})\right]=\langle R^{T}r_{i}r_{n}^{T}R\rangle_{Data}-\langle R^{T}r_{i}r_{n}^{T}R\rangle_{Model}
\]

\end_inset


\begin_inset Formula 
\[
\frac{\partial}{\partial R}\log\left[P(w_{n}|w_{1},...,w_{n-1})\right]=\langle\sum_{i=1}^{n-1}(r_{n}r_{i}^{T}RC^{(i)}+r_{i}r_{n}^{T}RC^{(i)T})+r_{n}b^{T}\rangle_{Data}-\langle\sum_{i=1}^{n-1}(r_{n}r_{i}^{T}RC^{(i)}+r_{i}r_{n}^{T}RC^{(i)T})+r_{n}b^{T}\rangle_{Model}
\]

\end_inset

where 
\begin_inset Formula $\langle r_{n}\rangle_{Model}$
\end_inset

 is the vector feature of the word predicted by the LBL model, i.e.
 the word 
\begin_inset Formula $w_{n}$
\end_inset

 of the dictionary with the highest predictive probability, and 
\begin_inset Formula $\langle r_{n}\rangle_{Data}$
\end_inset

 is the vector feature of the ground-truth word, i.e.
 the word originally in the sequence.
\end_layout

\begin_layout Standard
However, I (Léo) believe that this transformation is not adapted for musical
 representation.
 Indeed, to adapt the log-bilinear transformation to musical vectors would
 first imply that we map the standard representation (over the 88 pitches)
 on a one-hot encoding.
 This would imply a dictionary size of the number of possible combination,
 which is 
\begin_inset Formula $\begin{pmatrix}88\\
1
\end{pmatrix}+\begin{pmatrix}88\\
2
\end{pmatrix}+\begin{pmatrix}88\\
3
\end{pmatrix}+\begin{pmatrix}88\\
4
\end{pmatrix}=2.445.542$
\end_inset

 for a single instrument restricted to 4 voices and 
\begin_inset Formula $\begin{pmatrix}360\\
1
\end{pmatrix}+...+\begin{pmatrix}360\\
15
\end{pmatrix}>1\times10^{26}$
\end_inset

 for an orchestra of 15 instruments.
 This is a by considering a very restrictive case where each instrument
 can only play on two octaves (
\begin_inset Formula $360=15*24$
\end_inset

) and they all play a single note at the same time in the worst case (
\begin_inset Formula $1$
\end_inset

to
\begin_inset Formula $15$
\end_inset

).
 The size of the dictionary for the orchestral case prevent us from applying
 directly this technic in the context of automatic orchestration.
 However, we still believe that a better representation must be found in
 order to avoid the sparse vector we obtain with a simple pianoroll representati
on.
\end_layout

\begin_layout Paragraph
Voice leading : Partial permutations
\end_layout

\begin_layout Standard
The pianoroll representation does not suggest any link between time and
 harmony.
 However, the voice leading technic used from the medieval music to many
 contemporary jazzmen suggests an intimate relation between those two dimension.
 Being able to include those relations as a prior in our representation
 can be an interesting lead.
 The work presented in (REF MATTIA) rely on multiset theory to define a
 voice leading as a product of partial permutation matrices.
 If we consider a voice leading of 
\begin_inset Formula $n$
\end_inset

 voices, it can be seen as a sequence of chords defined for each time frame.
 Each chord can be represented by a multiset 
\begin_inset Formula $M$
\end_inset

 which is composed by a set and a multiplicity function 
\begin_inset Formula $(X_{M},\mu_{M})$
\end_inset

.
 The set enumerates the pitches present in the chord, and the multiplicity
 function translates the fact that two different voices can at a certain
 moment have the exact same pitch (e.g.
 a 4 voices voice leading collapse into a 3 voices one).
 If the next chord is represented by the multiset 
\begin_inset Formula $L=(X_{L},\mu_{L})$
\end_inset

, then the voice leading between 
\begin_inset Formula $M$
\end_inset

 and 
\begin_inset Formula $L$
\end_inset

 can be described by a partial permutation 
\begin_inset Formula $P$
\end_inset

 in the multiset 
\begin_inset Formula $M\cup L=(X_{M}\cup X_{L},\mu_{M\cup L})$
\end_inset

 where 
\begin_inset Formula $\mu_{M\cup L}=max(\mu_{M},\mu_{L})$
\end_inset

 (see article for details).
 
\end_layout

\begin_layout Standard
Those partial permutation matrices 
\begin_inset Formula $P$
\end_inset

 could then be used as an other data representation to feed a network.
 The first obvious step is transform those matrices into one dimensional
 representations.
 Considering aa sequence of partial permutation matrices 
\begin_inset Formula $P_{1}...P_{N}$
\end_inset

, we can define 
\begin_inset Formula $N$
\end_inset

 vectors 
\begin_inset Formula $\hat{p}_{1}...\hat{p}_{N}$
\end_inset

, corresponding to the N time frames, by simply concatenating the columns
 of the permutations matrices.
 Dynamics can then be added to this representation by introducing multiplicative
 coefficient instead of 0 and 1 in the matrices.
 For instance, if the transition between the 
\begin_inset Formula $i^{th}$
\end_inset

note and the 
\begin_inset Formula $j^{th}$
\end_inset

note exists, instead of describing it as 
\begin_inset Formula $p(i,j)=1$
\end_inset

, we would now describe it as 
\begin_inset Formula $p(i,j)=\frac{d(j,t+1)}{d(i,t)}$
\end_inset

 if 
\begin_inset Formula $d(i,t)\neq0$
\end_inset

 else 
\begin_inset Formula $p(i,j)=d(j,t+1)$
\end_inset

 where 
\begin_inset Formula $d(i,t)$
\end_inset

 is the dynamic of the note i at the time t.
 Another solution could be to add a vector of size the number of voices
 containing the dynamics.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
PROBLEME : c'est de la merde comme représentation...
 si 
\begin_inset Formula $d(i,t)=0$
\end_inset

, 
\begin_inset Formula $d(j,t+1)=1$
\end_inset

 n'a pas du tout le même sens que si 
\begin_inset Formula $d(i,t)=0,1$
\end_inset

 et 
\begin_inset Formula $d(j,t+1)=1$
\end_inset

 pourtant 
\begin_inset Formula $d(j,t+1)$
\end_inset

 a la même valeur dans les deux cas.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, some issues must still be resolved in order to use this representation.
 Indeed, if a voice leading occurs between two multisets 
\emph on
M
\emph default
 and 
\emph on
L, 
\emph default
the succession of permutation matrices does not give the successive sets
 
\emph on
L
\emph default
.
 An other network could be used to dig the next multiset given the previous,
 but this lead us back to the pianoroll representation...
\end_layout

\begin_layout Subsubsection
Dimensionality reduction
\end_layout

\begin_layout Standard
PCA POUR DONNEES DISCRETES ????
\end_layout

\begin_layout Paragraph
PCA
\end_layout

\begin_layout Standard
Principal Components Analysis (PCA) seeks to find the main axes of variations
 of a set of vectors 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 Hence, this will require to know their covariance matrix 
\begin_inset Formula ${\textstyle \Sigma}$
\end_inset

 which can be computed (if 
\begin_inset Formula $\mathbf{x}$
\end_inset

 has a mean of zero) by 
\begin_inset Formula 
\[
\Sigma=\frac{1}{n}\sum_{i=1}^{n}\mathbf{x}_{i}\mathbf{x}_{i}^{T}
\]

\end_inset

The principal directions of data variation are then defined by the principal
 eigenvectors of 
\begin_inset Formula ${\textstyle \Sigma}$
\end_inset

.
 Based on this set of vectors 
\begin_inset Formula $U=[u_{1},\ldots u_{m}]$
\end_inset

, we can rotate the original vector 
\begin_inset Formula $\mathbf{x}$
\end_inset

 to obtain a set represented in this new basis
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
x_{{\rm rot}}=U^{T}x
\]

\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset Formula $U$
\end_inset

 is an orthogonal matrix, we can go back and forth from this representation
 in rotated space (
\begin_inset Formula $x_{{\rm rot}}$
\end_inset

) to the original space by computing 
\begin_inset Formula $x=Ux_{{\rm rot}}$
\end_inset

 (as 
\begin_inset Formula $U$
\end_inset

 satisfies 
\begin_inset Formula $U^{T}U=UU^{T}=I$
\end_inset

)
\end_layout

\begin_layout Standard
The dimensionality reduction aspect implies that if we have 
\begin_inset Formula $n$
\end_inset

-dimensional vectors 
\begin_inset Formula $x\in\mathcal{R}^{n}$
\end_inset

 and want to obtain 
\begin_inset Formula $k$
\end_inset

-dimensional representation 
\begin_inset Formula $\tilde{x}\in\mathbb{R}^{k}$
\end_inset

 (where 
\begin_inset Formula $k<n$
\end_inset

), while incurring a minimum amount of error, we could keep only the first
 
\begin_inset Formula $k$
\end_inset

 components of 
\begin_inset Formula $x_{{\rm rot}}$
\end_inset

, which embeds the most important directions of variation.
 In order to 
\bar under
recover an approximation 
\begin_inset Formula $\hat{x}$
\end_inset

 of the original value of 
\begin_inset Formula $x$
\end_inset

 after this dimensionality reduction, we'd just multiply 
\begin_inset Formula $\tilde{x}\in\mathbb{R}^{k}$
\end_inset

 with the first 
\begin_inset Formula $k$
\end_inset

 columns of 
\begin_inset Formula $U$
\end_inset

.
\end_layout

\begin_layout Standard
As different values of 
\begin_inset Formula $k$
\end_inset

 will imply different amounts of information loss, we can analyze the percentage
 of variance retained by various values of 
\begin_inset Formula $k$
\end_inset

.
 In order to automatically find the number of principal components, we compute
 
\begin_inset Formula 
\[
\frac{\sum_{j=1}^{k}\lambda_{j}}{\sum_{j=1}^{n}\lambda_{j}}
\]

\end_inset


\end_layout

\begin_layout Standard
A common choice is to pick the 
\begin_inset Formula $k$
\end_inset

 principal components that retain 99% of the variance.
\end_layout

\begin_layout Paragraph
Stationarity
\end_layout

\begin_layout Standard
PCA requires that each of the features lie in a similar range (variances
 of different features are similar) and that their mean are close to zero
 (usually obtained through zero-mean unit-variance transform).
 However, estimating a separate mean and variance for each dimension of
 the input vector seems quite senseless as the statistics in any part of
 the input should be similar to that of any other part.
 This property is called 
\emph on
stationarity
\emph default
.
\end_layout

\begin_layout Paragraph
Whitening
\end_layout

\begin_layout Standard
The preprocessing step known as 
\emph on
whitening
\emph default
 (sometimes called 
\emph on
sphering
\emph default
) is closely related to PCA and based on the same hypothesis that the raw
 input has large redundancies stemming from highly correlated neighborhoods.
 Similarly to PCA, whitening seeks to remove these redundancies so that
 the learning algorithms are fed with training input in which the features
 have low amounts of correlation and all exhibit the same variance.
\end_layout

\begin_layout Standard
As the PCA already uncorrelates the features by providing a rotated version
 of the input, we just need to ensure that each of the features have unit
 variance.
 To do so, we can rescale each feature (principal dimension) obtained with
 PCA by the scale of its variance (
\begin_inset Formula $1/\sqrt{\lambda_{i}}$
\end_inset

).
 Formally, we can obtain the whitened version of the data 
\begin_inset Formula $x_{{\rm white}}\in\mathbb{R}^{n}$
\end_inset

 as follows 
\begin_inset Formula 
\[
x_{{\rm white}}^{i}=\frac{x_{{\rm rot}}^{i}}{\sqrt{\lambda_{i}}}
\]

\end_inset


\end_layout

\begin_layout Standard
The obtained 
\begin_inset Formula $x_{white}$
\end_inset

 is the whitened version of the input with their different dimensions being
 uncorrelated and having unit variance.
 As in some cases the eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

 can be very close to 0, the whitening step that divides rotated data by
 
\begin_inset Formula $\sqrt{\lambda_{i}}$
\end_inset

 might lead to numerical errors.
 Therefore, this scaling step is usually smoothed out by adding a small
 regularization constant 
\begin_inset Formula $\epsilon$
\end_inset

 to the eigenvalues before normalization 
\begin_inset Formula 
\[
x_{{\rm white}}^{i}=\frac{x_{{\rm rot}}^{i}}{\sqrt{\lambda_{i}+\epsilon}}
\]

\end_inset


\end_layout

\begin_layout Paragraph
ZCA Whitening
\end_layout

\begin_layout Standard
It turns out that the transformation matrix that can be appled to the data
 in order to obtain a unit covariance in all dimension (identity matrix)
 is not unique.
 In fact, any orthogonal matrix 
\begin_inset Formula $R$
\end_inset

 (that satisfies 
\begin_inset Formula $RR^{T}=R^{T}R=I$
\end_inset

) applied to the whitened data by 
\begin_inset Formula $Rx_{{\rm white}}$
\end_inset

 will lead to data with identity covariance.
 The ZCA whitening operation is performed by choosing 
\begin_inset Formula $R=U$
\end_inset

 and applying 
\begin_inset Formula 
\[
x_{{\rm ZCAwhite}}=Ux_{{\rm white}}
\]

\end_inset


\end_layout

\begin_layout Paragraph
Pre-processing parameters
\end_layout

\begin_layout Standard
When a variety of pre-processing operations are applied to training data,
 it is mandatory to apply the same preprocessing parameters for both the
 training and test phase.
 For instance, the rotation matrix 
\begin_inset Formula $U$
\end_inset

 computing using PCA on the unlabeled training data should be applied identicall
y to preprocess the labeled test data.
 Indeed the algorithm learns to exploit statistical regularities in this
 transformed (rotated) space.
\end_layout

\begin_layout Subsection
Optimizations
\end_layout

\begin_layout Standard
Most of the training procedure for neural networks are eventually resumed
 to an optimization problem, either to find the minimum of a cost function
 
\emph on
E 
\emph default
or, in energy models, lower the energy function around training examples.
 The functions to be optimized are most of the time continuous, non-linear
 and defined over large multidimensional spaces (the space of the parameters
 of the network).
 A first set of non-linear optimization technics make the assumption that
 the surface defined by the error function on the parameter space is smooth
 and convex.
 They indeed consist in following the steepest path on this surface.
 In this section, we will review the gradient descent algorithm, which is
 probably the most commonly used, and the many other that derived from it,
 such as adaptive algorithms and use of second order derivatives.
 
\end_layout

\begin_layout Standard
The recent trend is to build increasingly large neural network and the dimension
 of the parameter space over which optimization is performed becomes very
 high (REFERECNBE).
 Several consequences follows from this observation
\end_layout

\begin_layout Subsection
Hyper-parameters analysis
\end_layout

\begin_layout Standard

\series bold
NB NB : Should need a small ref per parameter : NB NB
\end_layout

\begin_layout Subsubsection
The learning rate
\end_layout

\begin_layout Standard
The learning rate controls the speed of learning.
 Although it would seem natural to set this parameter to the maximal value,
 this increases the risk of overshooting (where the update makes steps so
 wide that it can 
\begin_inset Quotes eld
\end_inset

jump over
\begin_inset Quotes erd
\end_inset

 the optimal values).
 Oppositely, setting a low learning rate will cause the algorithm to be
 very slow to converge.
 An efficient way to trade between these two aspects is to gradually decrease
 the learning rate over the training epochs (as the first iterations have
 a higher chance of lying afar from a local optimum) and average the weights
 update across various epochs to reduce the learning noise.
\end_layout

\begin_layout Subsubsection
Initial weights values
\end_layout

\begin_layout Standard
The initial values of the weights are crucial to the success of the learning.
 First, it is mandatory not to initialize all weights to zeros, as this
 will lead all error gradient to be identical across the computation units
 and, therefore, the units will all end up.
 Hence, any form of random initialization to small weights values also serve
 the purpose of 
\emph on
symmetry breaking
\emph default
.
 Then, additionnal prior can be embedded in the weight initialization.
 For instance, when a sparsity penalty is imposed on the hidden units with
 a target probability of 
\begin_inset Formula $t$
\end_inset

, it seems logical to initialize the hidden biases to 
\begin_inset Formula $log\left(\nicefrac{t}{\left(1-t\right)}\right)$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Momentum
\end_layout

\begin_layout Standard
Momentum has been proposed to speed up the learning procedure.
 The main idea behind the momentum is to rely on a notion of velocity 
\begin_inset Formula $v$
\end_inset

 which is incremented at each step by the estimated gradient of the parameters
 weighted by the learning rate and the previous velocity.
 Hence, the momentum smooth the learning direction by taking into account
 the gradient updates of previous mini-batches.
 Therefore, the velocity is expected to decay with time given that a momentum
 meta-parameter controls the influence of previous velocities in the current
 parameter update 
\begin_inset Formula 
\[
\Delta\theta_{i}\left(t\right)=v_{i}\left(t\right)=\alpha v_{i}\left(t-1\right)-\epsilon\frac{\partial E}{\partial\theta_{i}}\left(t\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Using the method of momentum shifts the direction of the parameters which
 is not that of steepest descent, making this method similar to conjugate
 gradient approaches.
\end_layout

\begin_layout Subsubsection
Weight decay
\end_layout

\begin_layout Standard
The weight decay penalty allows to avoid degeneracy caused by an unbounded
 weight magnitude.
 This allows to impose a regularization by avoiding equivalent weight updates
 and also prevents overfitting.
 However, as the sum of all weights in a very large network might become
 extremely strong (sum of millions of parameters), the weight decay coefficient
 should be kept rather small (to ensure that it does not overshadow the
 reconstruction error cost).
\end_layout

\begin_layout Subsubsection
Held-out validation data
\end_layout

\begin_layout Standard
In order to select the appropriate amount of epochs and other factors of
 success of the algorithm several stop criterion can be implemented.
 These usually rely on held-out validation data, which allows to evaluate
 the capacity of the trained network to generalize on novel data.
 
\end_layout

\begin_layout Subsubsection
Encouraging sparse hidden activities
\end_layout

\begin_layout Standard
It has been shown that discriminative performance can be improved by using
 sparse features, i.e.
 only a very small portion of the units are active for each given input
 vector 
\series bold
(REF: Nair and Hinton, 2009)
\series default
.
 The idea is that this sparsity would be the sign of the features being
 sufficiently 
\emph on
specific 
\emph default
(each object can be decomposed in an efficiently small number of parts).
 One of the major differences between auto-encoders and RBMs comes from
 this sparsity requirement.
 Indeed, RBMs should not require the addition of a sparsity regularization
 term to encourage sparse activities because their use of stochastic binary
 hidden units already acts as a very strong regularizer.
 However, it is possible to introduce an extra sparsity constraint to further
 control the learning objective of RBMs 
\begin_inset CommandInset citation
LatexCommand cite
key "lee2008sparse"

\end_inset

.
\end_layout

\begin_layout Standard
This objective by setting a desired 
\emph on
sparsity target 
\emph default
which indicates the desired percentage of activation of a unit over the
 complete training set.
 The sparsity penalty term is therefore calculated as the difference between
 the actual activation probability and this desired target.
 This can be estimated with 
\begin_inset Formula $q$
\end_inset

 being the mean activation probability and performing an exponentially decaying
 overage over the different mini-batches 
\begin_inset Formula 
\[
q_{new}=\lambda q_{old}+(1\text{−}\lambda)q_{current}
\]

\end_inset


\end_layout

\begin_layout Standard
The natural penalty measure to use is the cross entropy between the desired
 sparsity 
\begin_inset Formula $t$
\end_inset

 and actual distribution 
\begin_inset Formula $q$
\end_inset

 
\begin_inset Formula 
\[
S_{penalty}\propto-t.log\left(q\right)-\left(1-t\right)log\left(1-q\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This penalty shows the advantage of having a simple derivative for each
 unit.
 Finally, this penalty is usually scaled by a 
\emph on
sparsity cost 
\emph default
parameter which set the impact of the sparsity on the overall cost.
 In order to efficiently set this parameter, histograms of the activities
 and careful initialization of the weights can allow to reduce the initial
 impact of this sparsity in order to increase its cost, without having it
 interfere with the reconstruction objective.
\end_layout

\begin_layout Subsubsection
Number of hidden units
\end_layout

\begin_layout Standard
Recently, a growing consensus in the deep learning community seems to point
 out that the architecture of the networks may play the most important part
 in the success of the learning.
 It has even been shown that networks with random weights had highly correlated
 accuracy with trained networks with similar architecture on classification
 problems 
\begin_inset CommandInset citation
LatexCommand cite
key "saxe2011random"

\end_inset

.
 Hence, it appears that the network architecture and connection pattern
 might have a more crucial importance than parameter initialization.
\end_layout

\begin_layout Standard

\series bold
+ Turns this paragraph into COMPLEXITY / Width vs.
 Depth / E.L.M / Full essay on structure
\end_layout

\begin_layout Standard

\bar under
When learning generative models of high-dimensional data, however, it is
 the number of bits that it takes to specify a data vector that determines
 how much constraint each training case imposes on the parameters of the
 model.
 This can be several orders of magnitude greater than number of bits required
 to specify a label.
 This would allow 1000 globally connected hidden units.
 If the hidden units are locally connected or if they use weight-sharing,
 many more can be used.
\end_layout

\begin_layout Subsubsection
Varieties of contrastive divergence
\end_layout

\begin_layout Standard
It has been shown that using a single step of contrastive divergence (CD1)
 provides surprisingly good performance 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

.
 However, it might also makes sense to increase the number of Gibbs sampling
 up to 
\begin_inset Formula $n$
\end_inset

 steps (CDn) 
\begin_inset CommandInset citation
LatexCommand cite
key "carreira2005contrastive"

\end_inset

.
 It has been shown that a practical way to balance this number of steps
 is to gradually increase this 
\begin_inset Formula $n$
\end_inset

 as the weights grow 
\begin_inset CommandInset citation
LatexCommand cite
key "salakhutdinov2007restricted"

\end_inset

.
 Another even more different way of learning is called 
\emph on
persistent contrastive divergence 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "tieleman2008training"

\end_inset

, which use persistent chains (called 
\emph on
fantasy particles
\emph default
) to initialize the Gibbs Markov chain instead of resetting it to a datavector
 at ecah iteration.
 The learning is then done by computing the difference in pairwise statistics
 on a minibatch and the pairwise statistics on these persistent chains.
 This approach has been improved by adding a set of 
\begin_inset Quotes eld
\end_inset

fast weights
\begin_inset Quotes erd
\end_inset

 overlaid on the standard parameters 
\begin_inset CommandInset citation
LatexCommand cite
key "tieleman2009using"

\end_inset

.
\end_layout

\begin_layout Subsubsection
The size of a mini-batch
\end_layout

\begin_layout Standard
A common efficient way to speed up the learning process is to divide the
 complete training set into small “
\emph on
mini-batches
\emph default
” (a random small subset of the entire input dataset) and perform one gradient
 step per batch.
 This allows both to perform a lot larger number of gradient updates (which
 improves the exploration of the solution space), and also to reduce the
 complexity of a single step (allowing tractable matrix operations to run
 on GPU boards).
 However, this would imply that different sizes of minibatch could impact
 the grandient descent optimization.
 Hence, to avoid changing the learning rate concomitently with the size
 of a mini-batch, the gradient computed on a mini-batch is averaged by its
 size.
\end_layout

\begin_layout Standard
The optimal size of a mini-batch is often given by the number of classes.
 To reduce the sampling error when estimating the gradient for the whole
 training set, each mini-batch should contain at least one instance from
 each class 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2010practical"

\end_inset

.
\end_layout

\begin_layout Subsection
Parameter tuning search
\end_layout

\begin_layout Subsection
Monitoring and displaying
\end_layout

\begin_layout Subsubsection
Monitoring the learning
\end_layout

\begin_layout Standard
Monitoring the learning is usually done by simply plotting the evolution
 of the reconstruction error.
 Given the nature of the learning objective, it would seem logical to monitor
 the error between the data and the reconstructions performed by the network
 during learning.
 However, it turns out that the reconstruction error is a quite poor indicator
 of the learning progress.
 For AEs, the reconstruction error does not faithfully reflects the additionnal
 constraints such as weight decay or sparsity.
 Therefore, this error provides no information on the 
\emph on
quality 
\emph default
of the learning (for instance, the identity function would give a perfect
 reconstruction but a useless learning).
 For RBMs, the reconstruction error is not at all the function that CDn
 is trying to approximate.
 Hence, it confounds together the difference between distributions of data
 and equilibrium distribution and the mixing rate.
 Therefore, alternate types of display can give lot deeper insights on the
 behavior and evolution of learning in deep networks rather than simply
 monitoring the value of the reconstruction error.
 
\end_layout

\begin_layout Paragraph
Histograms
\end_layout

\begin_layout Standard
By plotting histograms of the weights and biases, it is possible to quickly
 detect a degeneracy in the learning.
 The same type of scrutinization can be done by plotting the histogram of
 increments in the parameters, as this shows if the learning perform strong
 updates of the parameters, and even if updates are done at all.
\end_layout

\begin_layout Paragraph
Specificity of the features
\end_layout

\begin_layout Standard
As we hope to discover specific features, in which an input is efficiently
 decomposed in a sparse number of hidden features (which is the point of
 sparsity penalties), we can monitor the activation patterns over the datasets.
 Hence, a two-dimensionnal display of the activation probabilities where
 each row represent a hidden unit and each column an example allows to directly
 see if some units are unused or convertly activated over a large part of
 the datasets (the same apply to examples).
 This can show both the 
\emph on
specificity 
\emph default
of the units but also their 
\emph on
certainty 
\emph default
towards the feature they learned.
\end_layout

\begin_layout Paragraph
Weights display
\end_layout

\begin_layout Standard
An interesting way to understand the inner behavior of the network and more
 importantly what features have been learned by the hidden units is to display
 their weights.
 Knowing the weights 
\begin_inset Formula $W_{ij}$
\end_inset

 learned by a hidden unit, we can compute the input which maximally activates
 this unit by 
\begin_inset Formula 
\[
x_{j}=\frac{W_{ij}}{\sqrt{\left\Vert W_{i}\right\Vert ^{2}}}
\]

\end_inset

The normalizing term comes from the fact that the input is constrained by
 the 
\begin_inset Formula $L^{2}$
\end_inset

 norm.
 By displaying this information with the same procedure that would be used
 for the raw input data, we can understand which feature each of the hidden
 unit is looking for.
 Indeed, these show what maximally activates the unit so in turn shows what
 feature has been learned through expressing the correlations they are seeking.
 Therefore, these type of display can be interpreted as a form of 
\emph on
receptive field
\emph default
.
\end_layout

\begin_layout Subsubsection
t-distributed Stochastic Neighbor Embedding (t-SNE)
\end_layout

\begin_layout Standard

\series bold
SUMMARIZE THE FIRST ARTICLE on t-SNE.
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Section
Applications
\end_layout

\begin_layout Standard
Deep learning approaches have been appliued to a whole range of machine
 learning problems 
\series bold
[xREFx]
\series default
, mainly in computer vision 
\series bold
[xREFx] 
\series default
but also in more disparate fields ranging from audio processing 
\begin_inset CommandInset citation
LatexCommand cite
key "yu2011deep"

\end_inset

 to natural language processing 
\series bold
[xREFx] 
\series default
and even automatic game playing 
\begin_inset CommandInset citation
LatexCommand cite
key "mnih2015human"

\end_inset

.
 Even though applications have been flourishing in the conventional analysis,
 recognition and classification fields, the scope of deep processing have
 been gradually extended 
\bar under
to include more human-centric tasks of interpretation, understanding, retrieval,
 mining, and user interface 
\series bold
[xREFx]
\end_layout

\begin_layout Standard
Stacked auto-encoders have been used for speech compression problem with
 the aim of fitting the data to a fixed number of bits while minimizing
 the reproduction error 
\begin_inset CommandInset citation
LatexCommand cite
key "deng2010binary"

\end_inset

.
 The pretraining step of the DBN is shown to be crucial for increasing the
 coding efficiency.
\end_layout

\begin_layout Standard

\bar under
In 
\begin_inset CommandInset citation
LatexCommand cite
key "nair20093d"

\end_inset

, Nair and Hinton developed a modified DBN where the top-layer model uses
 a third-order Boltzmann machine.
 
\series bold
+ FOR WHAT (or remove)
\end_layout

\begin_layout Standard
Another way to rely on DBNs and deep autoencoder is to consider their ability
 to transform the input data to a higher-level representation.
 This approach was investigated for document indexing and retrieval [
\series bold
xREFx
\series default
], [
\series bold
xREFx]
\series default
, where ut is shown that the deepest hidden variables transforming the input
 based on word count features provides strongly better representations of
 each document.
 Furthermore, similarity in this hidden representation mirrors the semantic
 similarity of text documents, which facilitates rapid document retrieval.
\end_layout

\begin_layout Section
Future directions
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "yu2011deep"

\end_inset

:
\end_layout

\begin_layout Standard

\bar under
We need to better understand the deep model and deep learning.
 Why is learning in deep models difficult? Why do the generative pretraining
 approaches seem to be effective empirically? Is it possible to change the
 underlining probabilistic models to make the training easier? Are there
 other more effective and theoretically sound approaches to learn deep models?
\end_layout

\begin_layout Standard

\bar under
We need to find better feature extraction models at each layer
\end_layout

\begin_layout Standard

\bar under
This suggests that the current Gaussian-Bernoulli layer is not powerful
 enough to extract important discriminative information from the features.
\end_layout

\begin_layout Standard

\bar under
Theory needs to be developed to guide the search of proper feature extraction
 models at each layer.
\end_layout

\begin_layout Standard

\bar under
It is necessary to develop more powerful discriminative optimization techniques.
\end_layout

\begin_layout Standard

\bar under
The features extracted at the generative pretraining phase seem to describe
 the underlining speech variations well but do not contain enough information
 to distinguish between different languages.
 A learning strategy that can extract discriminative features for those
 tasks is in need.
 Extracting discriminative features may also greatly reduce the model size
\end_layout

\begin_layout Standard

\bar under
We need to develop effective and scalable parallel algorithms to train deep
 models.
 The current optimization algorithm, which is based on the mini-batch stochastic
 gradient, is difficult to be parallelized over computers.
 To make deep learning techniques scalable to thousands of hours of speech
 data, for example, theoretically sound parallel learning algorithms need
 to be developed.
\end_layout

\begin_layout Standard

\bar under
We need to search for better approaches to use deep architectures for modeling
 sequential data.
 The existing approaches, such as DBNHMM and DBN-CRF, represent simplistic
 and poor temporal models in exploiting the power of DBNs.
 Models that can use DBNs in a more tightly integrated way and learning
 procedures that optimize the sequential criterion
\end_layout

\begin_layout Standard

\bar under
Developing adaptation techniques for deep models is necessary.
 Many conventional models such as GMMHMM have well-developed adaptation
 techniques that allow for these models to perform well under diverse and
 changing real-world environments
\end_layout

\begin_layout Standard

\bar under
\begin_inset CommandInset citation
LatexCommand cite
key "humphrey2013feature"

\end_inset

:
\end_layout

\begin_layout Standard

\bar under
some research in feature learning has shown it is possible to discover unexpecte
d attributes that are useful to well-worn problems
\end_layout

\begin_layout Standard

\bar under
An unexpected consequence of this work is the realization that certain feature
 detectors, learned from constant-Q representations, seem to encode distinct
 pitch intervals.
\end_layout

\begin_layout Standard

\bar under
the long-standing assumption in feature design for genre recognition is
 that spectral contour matters much more than harmonic information
\end_layout

\begin_layout Standard

\bar under
e significantly improved upon by adding a musically motivated sequence model
 after the pattern recognition stage to smooth classification
\end_layout

\begin_layout Standard

\bar under
the work presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "humphrey2012rethinking"

\end_inset

 adopts a slightly different view of chord recognition.
 Using a CNN to classify five-second tiles of constant-Q pitch spectra,
 an end-to-end chord recognition system is produced that considers context
 from input observation to output label.
 Figure 10 illustrates how receptive fields, or local feature extractors,
 of a convolutional network build abstract representations as the hierarchical
 composition of parts over time.
\end_layout

\begin_layout Standard

\bar under
A hierarchy of harmony: Operating on five-second CQT patches as an input
 (i), the receptive fields of a CNN encode local behavior in feature maps
 (ii) at higher levels.
 This process can then be repeated (iii), allowing the network to characterize
 high-level attributes as the combination of simpler parts.
 This abstract representation can then be transformed into a probability
 surface (iv) for classifying the input
\end_layout

\begin_layout Standard

\bar under
First and foremost, building context into the feature representation greatly
 reduces the need for post-filtering after classification.
 Therefore, this accuracy is achieved by a causal chord recognition system
\end_layout

\begin_layout Standard

\bar under
updating this perspective is through discussions like this one, by demystifying
 the proverbial “black box” and understanding what, how, and why these methods
 work.
 Additionally, reframing traditional problems in the viewpoint of deep learning
 serves as an established starting point to begin developing a good comprehensio
n of implementing and realizing these systems.
\end_layout

\begin_layout Standard

\bar under
formulate novel or alternative theoretical foundations.
\end_layout

\begin_layout Standard

\bar under
successful application of deep learning necessitates a thorough understanding
 of these methods and how to apply them to the problem at hand.
 Various design decisions, such as model selection, data pre-processing,
 and carefully choosing the building blocks of the system, can impact performanc
e on a continuum from negligible differences in overall results to whether
 or not training can, or will, converge to anything useful.
 Likewise, the same kind of intuition holds for adjusting the various hyperparam
eters —learning rate, regularizers, sparsity penalties
\end_layout

\begin_layout Standard

\bar under
the approach is overtly more abstract and conceptual, placing a greater
 emphasis on high-level decisions like the choice of network topology or
 appropriate loss function
\end_layout

\begin_layout Standard

\bar under
, it is prudent to recognize that the majority of progress has occurred
 in computer vision.
 While this gives our community an excellent starting point, there are many
 assumptions inherent to image processing that start to break down when
 working with audio signals.
\end_layout

\begin_layout Standard

\bar under
the strongest correlations in an image occur within local neighborhoods,
 and this knowledge is reflected in the architectural design.
 Local neighborhoods in frequency do not share the same relationship, so
 the natural question becomes, “what architectures do make sense for time-freque
ncy representations?”
\end_layout

\begin_layout Standard

\bar under
In turn, such approaches may subsequently provide insight into the latent
 features that inform musical judgements, or even lead to deployable systems
 that could adapt to the nuances of an individual.
\end_layout

\begin_layout Standard

\bar under
the datadriven prior that it leverages can be steered by creating specific
 distributions, e.g., learn separate priors for rock versus jazz.
 Finally, music signals provide an interesting setting in which to further
 explore the role of time in perceptual AI systems, and has the potential
 to influence other time-series domains like video or motion capture data.
\end_layout

\begin_layout Standard

\bar under
\begin_inset CommandInset citation
LatexCommand cite
key "rifai2011contractive"

\end_inset

:
\end_layout

\begin_layout Standard

\bar under
much remains to be done to understand the characteristics and theoretical
 advantages of the representations learned by a Restricted Boltzmann Machine
 
\begin_inset CommandInset citation
LatexCommand cite
key "hinton2006fast"

\end_inset

, an auto-encoder 
\begin_inset CommandInset citation
LatexCommand cite
key "bengio2007greedy"

\end_inset

, sparse coding 
\begin_inset CommandInset citation
LatexCommand cite
key "olshausen1997sparse,kavukcuoglu2009learning"

\end_inset

, or semi-supervised embedding 
\end_layout

\begin_layout Section
General infos
\end_layout

\begin_layout Standard
DARPA deep learning program, available at http://www.darpa.
 mil/ipto/solicit/baa/BAA-09-40_PIP.pdf).
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "deeplearning"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
